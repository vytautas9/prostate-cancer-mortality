{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reserach for:\n",
    "\n",
    "1. Applied Mathematics Msc master thesis at Kaunas university of technology.\n",
    "2. P000M015 Research Project 3 course at Kaunas university of technology, study programme - Applied Mathematics MSc.\n",
    "3. Research Council of Lithuania funded student research in their free time “Development and application of machine learning methods in assessing the risk of mortality of prostate cancer patients” (P-ST-22-28). (2022-2023).\n",
    "Project github repo - https://github.com/vytautas9/Tiriamasis_Projektas_3\n",
    "\n",
    "Author:\n",
    "Vytautas Kraujalis\n",
    "\n",
    "LinkedIn - https://www.linkedin.com/in/vytautaskraujalis/  \n",
    "Email - vytautas.kraujalis2@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import logging\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to cleaned data file\n",
    "dataFilePath = 'data/data_clean.pkl'\n",
    "trainFilePath = 'data/data_train.pkl'\n",
    "testFilePath = 'data/data_test.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll log some training execution informatio to a log file \n",
    "# to check if everything goes as expected\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='logs.log', mode='w')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read already prepared and saved data\n",
    "with open(dataFilePath, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Read already prepared and saved train/test datasets\n",
    "with open(trainFilePath, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "with open(testFilePath, 'rb') as f:\n",
    "    data_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for categorical data\n",
    "data_train = pd.get_dummies(data_train, columns=['clinical_stage', 'biopsy_gleason_gg', 'pathological_gleason_gg',\n",
    "                                'pathologic_stage', 'lni', 'surgical_margin_status', 'persistent_psa',\n",
    "                                'TRYSgrupes', 'PLNDO1'])\n",
    "data_test = pd.get_dummies(data_test, columns=['clinical_stage', 'biopsy_gleason_gg', 'pathological_gleason_gg',\n",
    "                                'pathologic_stage', 'lni', 'surgical_margin_status', 'persistent_psa',\n",
    "                                'TRYSgrupes', 'PLNDO1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train:  (1251, 40) \n",
      "\n",
      "Columns of train:  Index(['age', 'psa', 'biopsy_gleason', 'survival_months', 'pathologic_gleason',\n",
      "       'survival_months_bcr', 'survival_months_mts', 'patient_id', 'bcr',\n",
      "       'mts', 'death_from_other_causes', 'cancer_specific_mortality',\n",
      "       'clinical_stage_1', 'clinical_stage_2', 'clinical_stage_3',\n",
      "       'biopsy_gleason_gg_1', 'biopsy_gleason_gg_2', 'biopsy_gleason_gg_3',\n",
      "       'biopsy_gleason_gg_4', 'biopsy_gleason_gg_5',\n",
      "       'pathological_gleason_gg_1', 'pathological_gleason_gg_2',\n",
      "       'pathological_gleason_gg_3', 'pathological_gleason_gg_4',\n",
      "       'pathological_gleason_gg_5', 'pathologic_stage_0', 'pathologic_stage_1',\n",
      "       'pathologic_stage_2', 'lni_0.0', 'lni_1.0', 'lni_unknown',\n",
      "       'surgical_margin_status_0', 'surgical_margin_status_1',\n",
      "       'persistent_psa_0', 'persistent_psa_1', 'TRYSgrupes_0', 'TRYSgrupes_1',\n",
      "       'TRYSgrupes_2', 'PLNDO1_0', 'PLNDO1_1'],\n",
      "      dtype='object') \n",
      "\n",
      "Shape of test:  (313, 40) \n",
      "\n",
      "Columns of test:  Index(['age', 'psa', 'biopsy_gleason', 'survival_months', 'pathologic_gleason',\n",
      "       'survival_months_bcr', 'survival_months_mts', 'patient_id', 'bcr',\n",
      "       'mts', 'death_from_other_causes', 'cancer_specific_mortality',\n",
      "       'clinical_stage_1', 'clinical_stage_2', 'clinical_stage_3',\n",
      "       'biopsy_gleason_gg_1', 'biopsy_gleason_gg_2', 'biopsy_gleason_gg_3',\n",
      "       'biopsy_gleason_gg_4', 'biopsy_gleason_gg_5',\n",
      "       'pathological_gleason_gg_1', 'pathological_gleason_gg_2',\n",
      "       'pathological_gleason_gg_3', 'pathological_gleason_gg_4',\n",
      "       'pathological_gleason_gg_5', 'pathologic_stage_0', 'pathologic_stage_1',\n",
      "       'pathologic_stage_2', 'lni_0.0', 'lni_1.0', 'lni_unknown',\n",
      "       'surgical_margin_status_0', 'surgical_margin_status_1',\n",
      "       'persistent_psa_0', 'persistent_psa_1', 'TRYSgrupes_0', 'TRYSgrupes_1',\n",
      "       'TRYSgrupes_2', 'PLNDO1_0', 'PLNDO1_1'],\n",
      "      dtype='object') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train: ', data_train.shape, '\\n')\n",
    "print('Columns of train: ', data_train.columns, '\\n')\n",
    "print('Shape of test: ', data_test.shape, '\\n')\n",
    "print('Columns of test: ', data_test.columns, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explodes the provided \"df\" dataset based on provided survival column \"time\" and\n",
    "clips the data to be in a range [min_time; max_time] (). A new discrete survival column\n",
    "will be created with name set as variable \"time_discrete\". \"cum_event\" boolean determines\n",
    "if cumulative event column will be created or no.\n",
    "\n",
    "clip(lower, upper) function will help us create a new discrete survival time column. \n",
    "If we specify lower=1 and upper=200, patients who experienced event earlier than 200th \n",
    "month will only have records till their event, on other side, if a patient survived past \n",
    "200th month, we will clip this information and will only keep information about him til 200th month.\n",
    "Another example, if we specify lower=140 and upper=200, and if the person experienced event \n",
    "at 100th month, we will create records for him till 140th (lower boundary) month.\n",
    "\"\"\"\n",
    "def explode_data(df,max_time,time,target_column,min_time=1,\n",
    "                 time_discrete='survival_time_discrete',cum_event=False):\n",
    "    \n",
    "    logging.info('Exploding the data......')\n",
    "    logging.debug(f'Exploding will happen for target column: {target_column}')\n",
    "    logging.debug(f'Data will be exploded with min {min_time} and max {max_time} survival times')\n",
    "    logging.debug(f'Survival time range BEFORE exploding: min {df[time].min()}, max {df[time].max()}')\n",
    "\n",
    "    target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "    # We create a new time column and clip the data by provided min and max survival times\n",
    "    df[time_discrete] = df[time].clip(min_time,max_time).apply(range)\n",
    "\n",
    "    # Exploding the dataset with the created range value in new time column\n",
    "    data_exploded = df.explode(time_discrete)\n",
    "    data_exploded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # New column starts at 0, we'll increase each value by 1\n",
    "    data_exploded[time_discrete] = pd.to_numeric(data_exploded[time_discrete]) + 1\n",
    "    logging.debug(f'Survival time range AFTER exploding: min {data_exploded[time_discrete].min()}, max {data_exploded[time_discrete].max()}')\n",
    "\n",
    "    # New event column, which will indicate the last event date\n",
    "    data_exploded[target_column_discrete] = (data_exploded[time_discrete] >= data_exploded[time]) * pd.to_numeric(data_exploded[target_column])\n",
    "    \n",
    "    if cum_event == True:\n",
    "        logging.info('Cumulative event will be added.....')\n",
    "        target_column_cumulative = target_column + '_cumulative'\n",
    "\n",
    "        # Create new event column with duplicated event values from discrete column\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_discrete]\n",
    "        \n",
    "        # For cumulative events, after end_time we will have NA values, we'll replace those with event indicator\n",
    "        after_survival_time = data_exploded[time_discrete] > data_exploded[time]\n",
    "        data_exploded.loc[after_survival_time, target_column_discrete] = -1\n",
    "        data_exploded[target_column_discrete] = data_exploded[target_column_discrete].replace(-1,np.NaN)\n",
    "        data_exploded.loc[(after_survival_time & (data_exploded[target_column]==0)), target_column_cumulative] = -1\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_cumulative].replace(-1,np.NaN)\n",
    "\n",
    "    return data_exploded\n",
    "\n",
    "\n",
    "def get_hyperparameters(model):\n",
    "    \"\"\"\n",
    "    Function to get hyperparameters of a model into a dictionary\n",
    "    \"\"\"\n",
    "    hyperparameters = {}\n",
    "    for key in model.get_params():\n",
    "        hyperparameters[key] = model.get_params()[key]\n",
    "    return hyperparameters\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Trains a single model on \"df_train\" data with response column \"target_column\". Model will be \n",
    "trained on data ranging to \"max_time\" (max, inclusive) or \"min_time\" (mininum max time, inclusive)\n",
    "and before training, the dataset will be exploded. \n",
    "\"\"\"\n",
    "def train_model(model, df_train, target_column, time, max_time, min_time=1, is_homogenous_dataset=False, experiment_name=None):\n",
    "    df_train_copy = df_train.copy()\n",
    "    \n",
    "    # Extract model name\n",
    "    model_name = type(model).__name__\n",
    "    logging.info(f'Starting fitting a {model_name} model...')\n",
    "    \n",
    "    logging.debug(f'Survival time column to be used: {time}')\n",
    "\n",
    "    # Explode the dataset\n",
    "    df_train_copy_exploded = explode_data(df_train_copy, min_time=min_time, max_time=max_time, time=time, target_column=target_column)\n",
    "    logging.debug(f'Column names of exploded train set: {df_train_copy_exploded.columns}')\n",
    "    target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "    # Drop targets/features from feature set\n",
    "    x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "    X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "    y_train = df_train_copy_exploded[target_column_discrete]\n",
    "    \n",
    "    # Fit model to training data\n",
    "    logging.debug(f'Fitting will happen on those features: {X_train.columns}')\n",
    "    logging.debug(f'Fitting response variable: {y_train.name}')\n",
    "    model.fit(X_train, y_train)\n",
    "    logging.info(f'Finished fitting.')\n",
    "\n",
    "    # Save model\n",
    "    if is_homogenous_dataset:\n",
    "        main_path = 'results/homogenous_data'\n",
    "    else:\n",
    "        main_path = 'results/non_homogenous_data'\n",
    "    model_path = f'{main_path}/{target_column}/{max_time}/{model_name}'\n",
    "    if experiment_name is None:\n",
    "        model_path = model_path + '/defaultExperiment'\n",
    "    else:\n",
    "        model_path = model_path + f'/{experiment_name}'\n",
    "    logging.info(f'Saving model to \"{model_path}\"...')\n",
    "    # Create directory if does not exist\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(f'{model_path}/model.sav', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    logging.info(f'Finished saving model.')\n",
    "    \n",
    "    logging.info(f'Getting model hyperparameters...')\n",
    "    hyperparameters = get_hyperparameters(model)\n",
    "    with open(f'{model_path}/hyperparameters.json', 'w') as f:\n",
    "        json.dump(hyperparameters, f, indent=4)\n",
    "    logging.info(f'Finished saving hyperparameters.')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given an exploded dataset with instant mortality probabilities \"event_probability_column\"\n",
    "and \"id_column\" for grouping (optional), cumulative hazard column will be calculated\n",
    "\"\"\"\n",
    "def cumulative_hazard(df, event_probability_column, id_column):\n",
    "    data_copy = df.copy()\n",
    "    logging.info('Starting calculated cumulative hazards......')\n",
    "    if id_column is not None:\n",
    "        logging.debug(f'Cumulative hazards will be grouped by: {id_column}')\n",
    "        data_copy = data_copy[ [id_column, event_probability_column] ]\n",
    "    else:\n",
    "        logging.warning('Cumulative hazards will NOT be grouped by any ID column.')\n",
    "        data_copy = data_copy[ [event_probability_column] ]\n",
    "    data_copy['negative_log_prob'] = np.log( 1 - data_copy[event_probability_column] )\n",
    "    if id_column is not None:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy.groupby(id_column)['negative_log_prob'].transform(pd.Series.cumsum))\n",
    "    else:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy['negative_log_prob'].cumsum())\n",
    "    return data_copy['cumulative_hazard']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given non exploded dataset, explodes the dataset based on \"max_time\", lower boundary for exploding\n",
    "a dataset will be the same as upper, so for each patient we will create \"max_time\" records.\n",
    "Adds predictend instant mortality probabilities as well as cumulative ones.\n",
    "\"\"\"\n",
    "def add_predict_probabilities(df, model, max_time, target_column, time, is_homogenous_dataset=False):\n",
    "    logging.info('Starting addition of predicted probabilities......')\n",
    "\n",
    "    df_exploded = explode_data(df, max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "\n",
    "    x_columns_to_drop = [target_column, target_column+'_discrete', target_column+'_cumulative', 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "    X_df = df_exploded.drop(x_columns_to_drop, axis=1)\n",
    "\n",
    "    # probabilities\n",
    "    y_pred = model.predict_proba(X_df)[:,1]\n",
    "    df_exploded['mortality_instant_prob'] = y_pred\n",
    "\n",
    "    # Cumulative hazard for each patient\n",
    "    df_exploded['cumulative_hazard'] = cumulative_hazard(df_exploded,'mortality_instant_prob','patient_id')\n",
    "    \n",
    "    # TODO - homogenous dataset\n",
    "    # if is_homogenous_dataset:\n",
    "        \n",
    "    #     # get adjustment parameters\n",
    "    #     match target_column:\n",
    "    #         case 'cancer_specific_mortality':\n",
    "    #             pi_0 = pi_0_csm\n",
    "    #             pi_1 = pi_1_csm\n",
    "    #             rho_0 = rho_0_csm\n",
    "    #             rho_1 = rho_1_csm\n",
    "    #         case 'death_from_other_causes':\n",
    "    #             pi_0 = pi_0_doc\n",
    "    #             pi_1 = pi_1_doc\n",
    "    #             rho_0 = rho_0_doc\n",
    "    #             rho_1 = rho_1_doc\n",
    "    #         case 'mts':\n",
    "    #             pi_0 = pi_0_mts\n",
    "    #             pi_1 = pi_1_mts\n",
    "    #             rho_0 = rho_0_mts\n",
    "    #             rho_1 = rho_1_mts\n",
    "    #         case 'bcr':\n",
    "    #             pi_0 = pi_0_bcr\n",
    "    #             pi_1 = pi_1_bcr\n",
    "    #             rho_0 = rho_0_bcr\n",
    "    #             rho_1 = rho_1_bcr\n",
    "    #         case _:\n",
    "    #             pi_0 = None\n",
    "    #             pi_1 = None\n",
    "    #             rho_0 = None\n",
    "    #             rho_1 = None\n",
    "        \n",
    "        \n",
    "    #     # adjusted probabilities\n",
    "    #     df_exploded['mortality_instant_prob_adjusted'] = \\\n",
    "    #         (df_exploded.mortality_instant_prob*(pi_1/rho_1)) / \\\n",
    "    #         ((1-df_exploded.mortality_instant_prob)*(pi_0/rho_0) + \\\n",
    "    #          df_exploded.mortality_instant_prob*(pi_1/rho_1))\n",
    "        \n",
    "    #     # Cumulative hazard for each patient (adjusted)\n",
    "    #     df_exploded['cumulative_hazard_adjusted'] = cumulative_hazard(df_exploded,'mortality_instant_prob_adjusted','patient_id')\n",
    "    \n",
    "    return df_exploded\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Wrapper for all above functions to be executed at once. Runs an experiment for single model - single slice of trainable months.\n",
    "\"\"\"\n",
    "def run_experiment(df_train, df_test, model, max_time, target_column, min_time=1, is_homogenous_dataset=False, experiment_name=None):\n",
    "    \n",
    "    model_name = type(model).__name__\n",
    "\n",
    "    logging.info('\\n|-------------------------------------------------------|')\n",
    "    logging.info(f'Running an experiment on model: {model_name}, experiment name: {experiment_name}')\n",
    "    logging.debug(f'Training dataset type: {\"homogenous\" if is_homogenous_dataset else \"non-homogenous\"}')\n",
    "    logging.debug(f'Target column: {target_column}')\n",
    "    logging.debug(f'Min trainalbe time: {min_time}, Max trainable time: {max_time}')\n",
    "\n",
    "    # mts and bcr have different survival months columns\n",
    "    match target_column:\n",
    "        case 'mts':\n",
    "            time = 'survival_months_mts'\n",
    "        case 'bcr':\n",
    "            time = 'survival_months_bcr'\n",
    "        case _:\n",
    "            time = 'survival_months'\n",
    "\n",
    "    model = train_model(model=model, df_train=df_train, target_column=target_column,\n",
    "                   max_time=max_time, min_time=min_time, is_homogenous_dataset=is_homogenous_dataset, \n",
    "                   experiment_name=experiment_name, time=time)\n",
    "    \n",
    "    # Test on training data\n",
    "    df_train_predicted = add_predict_probabilities(df_train, model=model, max_time=max_time, \n",
    "                                                     target_column=target_column, time=time)\n",
    "    \n",
    "    # Test on testing data\n",
    "    df_test_predicted = add_predict_probabilities(df_test, model=model, max_time=max_time, \n",
    "                                                    target_column=target_column, time=time,\n",
    "                                                  is_homogenous_dataset=is_homogenous_dataset)\n",
    "    \n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    test_auc_stats = []\n",
    "    test_adjusted_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_predicted['survival_time_discrete'] == month) & pd.notna(df_train_predicted[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_predicted[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "        # --- Testing data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_test_predicted['survival_time_discrete'] == month) & pd.notna(df_test_predicted[target_column+'_cumulative'])\n",
    "        sub_dat = df_test_predicted[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        test_auc_stats.append(auc_stat)\n",
    "        \n",
    "        # --- Testing adjusted data ---\n",
    "        # TODO - homogenous dataset\n",
    "        # if is_homogenous_dataset:\n",
    "        #     # Selecting a subset of data based on the months\n",
    "        #     select = (df_test_predicted['survival_time_discrete'] == month) & pd.notna(df_test_predicted[target_column+'_cumulative'])\n",
    "        #     sub_dat = df_test_predicted[select]\n",
    "\n",
    "        #     # If in the sliced data there's a event, calculate AUC metric,\n",
    "        #     # otherwise assign NaN value\n",
    "        #     if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "        #         fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard_adjusted'])\n",
    "        #         auc_stat = auc(fpr, tpr)\n",
    "        #     else:\n",
    "        #         auc_stat = float('NaN')\n",
    "        #     test_adjusted_auc_stats.append(auc_stat)\n",
    "            \n",
    "\n",
    "    if is_homogenous_dataset:\n",
    "        # auc_stats = pd.DataFrame(zip(months, train_auc_stats, test_auc_stats, test_adjusted_auc_stats),\n",
    "        #                      columns=['month', 'train_auc', 'test_auc', 'test_adjusted_auc'])\n",
    "        # TODO - homogenous dataset\n",
    "        pass\n",
    "    else:\n",
    "        auc_stats = pd.DataFrame(zip(months, train_auc_stats, test_auc_stats),\n",
    "                             columns=['month', 'train_auc', 'test_auc'])\n",
    "    \n",
    "    \n",
    "    # Save the auc statistics\n",
    "    if is_homogenous_dataset:\n",
    "        main_path = 'results/homogenous_data'\n",
    "    else:\n",
    "        main_path = 'results/non_homogenous_data'\n",
    "    model_path = f'{main_path}/{target_column}/{max_time}/{model_name}'\n",
    "    if experiment_name is None:\n",
    "        model_path = model_path + '/defaultExperiment'\n",
    "    else:\n",
    "        model_path = model_path + f'/{experiment_name}'\n",
    "    logging.info(f'Saving results to \"{model_path}\"...')\n",
    "    # Create directory if does not exist\n",
    "    Path(model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(f'{model_path}/auc_statistics.pkl', 'wb') as f:\n",
    "        pickle.dump(auc_stats, f)\n",
    "    \n",
    "    logging.info('\\n|-------------------------------------------------------|')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function to read a single experiment from file\n",
    "\"\"\"\n",
    "def read_experiment(model_name, target_column, max_time, is_homogenous_dataset=False, experiment_name=None):\n",
    "    if is_homogenous_dataset:\n",
    "        main_path = 'results/homogenous_data'\n",
    "    else:\n",
    "        main_path = 'results/non_homogenous_data'\n",
    "        \n",
    "    model_path = f'{main_path}/{target_column}/{max_time}/{model_name}'\n",
    "\n",
    "    if experiment_name is None:\n",
    "        model_path = model_path + '/defaultExperiment'\n",
    "    else:\n",
    "        model_path = model_path + f'/{experiment_name}'\n",
    "        \n",
    "    # Read model\n",
    "    with open(f'{model_path}/model.sav', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    # Read auc statistic\n",
    "    with open(f'{model_path}/auc_statistics.pkl', 'rb') as f:\n",
    "        auc_stats = pickle.load(f)\n",
    "    return model, auc_stats\n",
    "\n",
    "\n",
    "def run_experiment_wrapper(df_train_par, df_test_par, model_dict, experiment_name=None):\n",
    "    # List of columns names which will be dropped from feature set before fitting the model\n",
    "    target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "    # list of months which will be tried out\n",
    "    trainable_months = list(range(24, 225, 12))\n",
    "\n",
    "    is_homogenous_dataset = False\n",
    "\n",
    "    for target_column in ['cancer_specific_mortality', 'death_from_other_causes', 'mts', 'bcr']:\n",
    "        print(f'target_column: {target_column}')\n",
    "\n",
    "        target_columns_drop = target_columns.copy()\n",
    "        target_columns_drop.remove(target_column)\n",
    "        df_train = df_train_par.copy().drop(target_columns_drop, axis=1)\n",
    "        df_test = df_test_par.copy().drop(target_columns_drop, axis=1)\n",
    "\n",
    "        for month in trainable_months:\n",
    "            print(f'\\tTrainable month: {month}')\n",
    "\n",
    "            # Define the Random Forest model\n",
    "            match target_column:\n",
    "                case 'cancer_specific_mortality':\n",
    "                    model = model_dict['cancer_specific_mortality']\n",
    "                case 'death_from_other_causes':\n",
    "                    model = model_dict['death_from_other_causes']\n",
    "                case 'mts':\n",
    "                    model = model_dict['mts']\n",
    "                case 'bcr':\n",
    "                    model = model_dict['bcr']\n",
    "                case _:\n",
    "                    pass\n",
    "            model_name = type(model).__name__\n",
    "            \n",
    "            # Run the experiment\n",
    "            run_experiment(df_train=df_train, df_test=df_test, model=model, \n",
    "                        max_time=month, target_column=target_column, \n",
    "                        is_homogenous_dataset=is_homogenous_dataset,\n",
    "                        experiment_name=experiment_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_column: cancer_specific_mortality\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: death_from_other_causes\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: mts\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: bcr\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": LogisticRegression(solver='liblinear', random_state=0),\n",
    "    \"death_from_other_causes\": LogisticRegression(solver='liblinear', random_state=0),\n",
    "    \"bcr\": LogisticRegression(solver='liblinear', random_state=0),\n",
    "    \"mts\": LogisticRegression(solver='liblinear', random_state=0)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=400, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.1),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=400, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.1),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=400, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.1),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=400, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.1)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 001:  \n",
    "\n",
    "CSM: {'criterion': 0,\n",
    " 'max_depth': 5.498136049182303,\n",
    " 'max_features': 0,\n",
    " 'min_samples_leaf': 0.007264818733396294,\n",
    " 'min_samples_split': 0.05282217977945802,\n",
    " 'n_estimators': 426.84675889453035}\n",
    "\n",
    " DOC: {'criterion': 1,\n",
    " 'max_depth': 6.308941631995079,\n",
    " 'max_features': 0,\n",
    " 'min_samples_leaf': 0.25256851781175843,\n",
    " 'min_samples_split': 0.10004228078151234,\n",
    " 'n_estimators': 397.0599160292858}\n",
    "\n",
    " MTS: {'criterion': 1,\n",
    " 'max_depth': 16.810243822100986,\n",
    " 'max_features': 0,\n",
    " 'min_samples_leaf': 0.03125048473423414,\n",
    " 'min_samples_split': 0.22516444376211764,\n",
    " 'n_estimators': 353.6896846350636}\n",
    "\n",
    " BCR: {'criterion': 0,\n",
    " 'max_depth': 16.103488534169735,\n",
    " 'max_features': 2,\n",
    " 'min_samples_leaf': 0.05973709304919683,\n",
    " 'min_samples_split': 0.09831750690258467,\n",
    " 'n_estimators': 397.0952759650272}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=426, random_state=0, criterion='entropy', max_depth=5,\n",
    "                              max_samples=0.3, max_features='sqrt', min_samples_leaf=0.007, min_samples_split=0.05),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=397, random_state=0, criterion='gini', max_depth=6,\n",
    "                              max_samples=0.3, max_features='sqrt', min_samples_leaf=0.25, min_samples_split=0.1),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=397, random_state=0, criterion='entropy', max_depth=16,\n",
    "                              max_samples=0.3, max_features=None, min_samples_leaf=0.06, min_samples_split=0.1),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=354, random_state=0, criterion='gini', max_depth=17,\n",
    "                              max_samples=0.3, max_features='sqrt', min_samples_leaf=0.03, min_samples_split=0.22)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 001')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment A:  \n",
    "{'n_estimators': 350,\n",
    " 'max_samples': 0.8,\n",
    " 'max_features': 0.7000000000000001,\n",
    " 'max_depth': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=350, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.8, max_features=0.7),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=350, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.8, max_features=0.7),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=350, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.8, max_features=0.7),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=350, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.8, max_features=0.7)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment A')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment B:  \n",
    "{'n_estimators': 200,\n",
    " 'min_samples_leaf': 5,\n",
    " 'max_samples': 0.3,\n",
    " 'max_features': 0.5,\n",
    " 'max_depth': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=200, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.5, min_samples_leaf=5),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=200, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.5, min_samples_leaf=5),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=200, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.5, min_samples_leaf=5),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=200, random_state=0, criterion='entropy', max_depth=3,\n",
    "                              max_samples=0.3, max_features=0.5, min_samples_leaf=5)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment B')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment C:  \n",
    "{'criterion': 1,\n",
    " 'max_depth': 18.61491475813544,\n",
    " 'max_features': 2,\n",
    " 'min_samples_leaf': 5.4715419273109606e-05,\n",
    " 'min_samples_split': 0.3178996881755699,\n",
    " 'n_estimators': 239.84475133029923}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=240, random_state=0, criterion='gini', max_depth=19,\n",
    "                              max_features=None, min_samples_leaf=5.4715419273109606e-05, min_samples_split=0.3178996881755699),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=240, random_state=0, criterion='gini', max_depth=19,\n",
    "                              max_features=None, min_samples_leaf=5.4715419273109606e-05, min_samples_split=0.3178996881755699),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=240, random_state=0, criterion='gini', max_depth=19,\n",
    "                              max_features=None, min_samples_leaf=5.4715419273109606e-05, min_samples_split=0.3178996881755699),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=240, random_state=0, criterion='gini', max_depth=19,\n",
    "                              max_features=None, min_samples_leaf=5.4715419273109606e-05, min_samples_split=0.3178996881755699)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment C')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 002:\n",
    "\n",
    "CSM: {'criterion': 0,\n",
    " 'max_depth': 17.937192102303005,\n",
    " 'max_features': 2,\n",
    " 'min_samples_leaf': 0.03932576931296611,\n",
    " 'min_samples_split': 0.025730604369255006,\n",
    " 'n_estimators': 494.2199824963416}\n",
    "\n",
    " DOC: {'criterion': 0,\n",
    " 'max_depth': 15.99874407715512,\n",
    " 'max_features': 2,\n",
    " 'min_samples_leaf': 0.05467350337454149,\n",
    " 'min_samples_split': 0.5886741613999928,\n",
    " 'n_estimators': 376.7957625705628}\n",
    "\n",
    " MTS: {'criterion': 1,\n",
    " 'max_depth': 13.577996616687528,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.036805117857165615,\n",
    " 'min_samples_split': 0.3255535986892048,\n",
    " 'n_estimators': 149.6122350278912}\n",
    "\n",
    " BCR: {'criterion': 1,\n",
    " 'max_depth': 9.362170539012933,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.04889852261499672,\n",
    " 'min_samples_split': 0.2490075040108427,\n",
    " 'n_estimators': 160.7620972043769}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=494, random_state=0, criterion='entropy', max_depth=18,\n",
    "                              max_features=None, min_samples_leaf=0.04, min_samples_split=0.025),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=378, random_state=0, criterion='entropy', max_depth=16,\n",
    "                              max_features=None, min_samples_leaf=0.05, min_samples_split=0.59),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=161, random_state=0, criterion='gini', max_depth=9,\n",
    "                              max_features='log2', min_samples_leaf=0.05, min_samples_split=0.25),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=150, random_state=0, criterion='gini', max_depth=14,\n",
    "                              max_features='log2', min_samples_leaf=0.04, min_samples_split=0.33)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 002')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 003:\n",
    "\n",
    "CSM: {'criterion': 0,\n",
    " 'max_depth': 5.008074817422085,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.0348778696429207,\n",
    " 'min_samples_split': 0.10209281579299412,\n",
    " 'n_estimators': 334.0017140265728}\n",
    "\n",
    " DOC: {'criterion': 0,\n",
    " 'max_depth': 15.99874407715512,\n",
    " 'max_features': 2,\n",
    " 'min_samples_leaf': 0.05467350337454149,\n",
    " 'min_samples_split': 0.5886741613999928,\n",
    " 'n_estimators': 376.7957625705628}\n",
    "\n",
    " MTS: {'criterion': 1,\n",
    " 'max_depth': 13.577996616687528,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.036805117857165615,\n",
    " 'min_samples_split': 0.3255535986892048,\n",
    " 'n_estimators': 149.6122350278912}\n",
    "\n",
    " BCR: {'criterion': 1,\n",
    " 'max_depth': 9.362170539012933,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.04889852261499672,\n",
    " 'min_samples_split': 0.2490075040108427,\n",
    " 'n_estimators': 160.7620972043769}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=334, random_state=0, criterion='entropy', max_depth=5,\n",
    "                              max_features='log2', min_samples_leaf=0.03, min_samples_split=0.1),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=378, random_state=0, criterion='entropy', max_depth=16,\n",
    "                              max_features=None, min_samples_leaf=0.05, min_samples_split=0.59),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=161, random_state=0, criterion='gini', max_depth=9,\n",
    "                              max_features='log2', min_samples_leaf=0.05, min_samples_split=0.25),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=150, random_state=0, criterion='gini', max_depth=14,\n",
    "                              max_features='log2', min_samples_leaf=0.04, min_samples_split=0.33)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 003')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 004:\n",
    "\n",
    "CSM: {'criterion': 0,\n",
    " 'max_depth': 5.008074817422085,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.0348778696429207,\n",
    " 'min_samples_split': 0.10209281579299412,\n",
    " 'n_estimators': 334.0017140265728}\n",
    "\n",
    " DOC: {'criterion': 0,\n",
    " 'max_depth': 15.99874407715512,\n",
    " 'max_features': 2,\n",
    " 'min_samples_leaf': 0.05467350337454149,\n",
    " 'min_samples_split': 0.5886741613999928,\n",
    " 'n_estimators': 376.7957625705628}\n",
    "\n",
    " MTS: {'criterion': 1,\n",
    " 'max_depth': 13.577996616687528,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.036805117857165615,\n",
    " 'min_samples_split': 0.3255535986892048,\n",
    " 'n_estimators': 149.6122350278912}\n",
    "\n",
    " BCR: {'criterion': 1,\n",
    " 'max_depth': 9.362170539012933,\n",
    " 'max_features': 1,\n",
    " 'min_samples_leaf': 0.04889852261499672,\n",
    " 'min_samples_split': 0.2490075040108427,\n",
    " 'n_estimators': 160.7620972043769}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": RandomForestClassifier(n_estimators=334, random_state=0, criterion='entropy', max_depth=5,\n",
    "                              max_features='log2', min_samples_leaf=0.03, min_samples_split=0.1),\n",
    "    \"death_from_other_causes\": RandomForestClassifier(n_estimators=378, random_state=0, criterion='entropy', max_depth=16,\n",
    "                              max_features=None, min_samples_leaf=0.05, min_samples_split=0.59),\n",
    "    \"bcr\": RandomForestClassifier(n_estimators=161, random_state=0, criterion='gini', max_depth=9,\n",
    "                              max_features='log2', min_samples_leaf=0.05, min_samples_split=0.25),\n",
    "    \"mts\": RandomForestClassifier(n_estimators=150, random_state=0, criterion='gini', max_depth=14,\n",
    "                              max_features='log2', min_samples_leaf=0.04, min_samples_split=0.33)\n",
    "}\n",
    "\n",
    "df_copy = data_train.copy().drop(['TRYSgrupes_0', 'TRYSgrupes_1', 'TRYSgrupes_2'], axis=1)\n",
    "df_test_copy = data_test.copy().drop(['TRYSgrupes_0', 'TRYSgrupes_1', 'TRYSgrupes_2'], axis=1)\n",
    "\n",
    "run_experiment_wrapper(df_copy.copy(), df_test_copy.copy(), model_dict, experiment_name='Experiment 004')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.2, gamma=0.1, learning_rate=0.1,\n",
    "                      max_depth=4, n_estimators=500, subsample = 0.2,\n",
    "                     min_child_weight=3, scale_pos_weight=0.7, \n",
    "                      reg_lambda=1, reg_alpha=0.001),\n",
    "    \"death_from_other_causes\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.2, gamma=0.1, learning_rate=0.1,\n",
    "                      max_depth=4, n_estimators=500, subsample = 0.2,\n",
    "                     min_child_weight=3, scale_pos_weight=0.7, \n",
    "                      reg_lambda=1, reg_alpha=0.001),\n",
    "    \"bcr\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.2, gamma=0.1, learning_rate=0.1,\n",
    "                      max_depth=4, n_estimators=500, subsample = 0.2,\n",
    "                     min_child_weight=3, scale_pos_weight=0.7, \n",
    "                      reg_lambda=1, reg_alpha=0.001),\n",
    "    \"mts\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.2, gamma=0.1, learning_rate=0.1,\n",
    "                      max_depth=4, n_estimators=500, subsample = 0.2,\n",
    "                     min_child_weight=3, scale_pos_weight=0.7, \n",
    "                      reg_lambda=1, reg_alpha=0.001)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 001:  \n",
    "\n",
    "CSM: {'colsample_bytree': 0.9,\n",
    " 'gamma': 0.0,\n",
    " 'learning_rate': 0.2,\n",
    " 'max_depth': 18.0,\n",
    " 'min_child_weight': 2.0,\n",
    " 'n_estimators': 494.0763574289686,\n",
    " 'reg_alpha': 0.011624533771820083,\n",
    " 'reg_lambda': 2.3750241072036045,\n",
    " 'scale_pos_weight': 0.884642762544353,\n",
    " 'subsample': 1.0}\n",
    "\n",
    " DOC: {'colsample_bytree': 0.65,\n",
    " 'gamma': 0.25,\n",
    " 'learning_rate': 0.17500000000000002,\n",
    " 'max_depth': 11.0,\n",
    " 'min_child_weight': 1.0,\n",
    " 'n_estimators': 618.9569732240903,\n",
    " 'reg_alpha': 0.00460840736862405,\n",
    " 'reg_lambda': 1.5957720013269137,\n",
    " 'scale_pos_weight': 0.9929006236241394,\n",
    " 'subsample': 0.7000000000000001}\n",
    "\n",
    " MTS: {'colsample_bytree': 0.55,\n",
    " 'gamma': 0.05,\n",
    " 'learning_rate': 0.15000000000000002,\n",
    " 'max_depth': 13.0,\n",
    " 'min_child_weight': 2.0,\n",
    " 'n_estimators': 535.373615733675,\n",
    " 'reg_alpha': 0.012615056264249302,\n",
    " 'reg_lambda': 0.7584283803105774,\n",
    " 'scale_pos_weight': 0.9095133867383003,\n",
    " 'subsample': 0.65}\n",
    "\n",
    " BCR: {'colsample_bytree': 0.7000000000000001,\n",
    " 'gamma': 0.1,\n",
    " 'learning_rate': 0.2,\n",
    " 'max_depth': 19.0,\n",
    " 'min_child_weight': 1.0,\n",
    " 'n_estimators': 320.2127746661102,\n",
    " 'reg_alpha': 0.06919873757682321,\n",
    " 'reg_lambda': 0.01593110711960384,\n",
    " 'scale_pos_weight': 0.4838101096249437,\n",
    " 'subsample': 0.7000000000000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.9, gamma=0.01, learning_rate=0.2,\n",
    "                      max_depth=18, n_estimators=494, subsample = 1,\n",
    "                     min_child_weight=2, scale_pos_weight=0.88, \n",
    "                      reg_lambda=2.37, reg_alpha=0.01),\n",
    "    \"death_from_other_causes\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.65, gamma=0.25, learning_rate=0.175,\n",
    "                      max_depth=11, n_estimators=619, subsample = 0.7,\n",
    "                     min_child_weight=1, scale_pos_weight=0.99, \n",
    "                      reg_lambda=1.6, reg_alpha=0.005),\n",
    "    \"bcr\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.7, gamma=0.1, learning_rate=0.2,\n",
    "                      max_depth=19, n_estimators=320, subsample = 0.7,\n",
    "                     min_child_weight=1, scale_pos_weight=0.48, \n",
    "                      reg_lambda=0.02, reg_alpha=0.07),\n",
    "    \"mts\": XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree',\n",
    "                      colsample_bytree=0.55, gamma=0.05, learning_rate=0.15,\n",
    "                      max_depth=13, n_estimators=535, subsample = 0.65,\n",
    "                     min_child_weight=2, scale_pos_weight=0.91, \n",
    "                      reg_lambda=0.76, reg_alpha=0.01)\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 001')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": MLPClassifier(),\n",
    "    \"death_from_other_causes\": MLPClassifier(),\n",
    "    \"bcr\": MLPClassifier(),\n",
    "    \"mts\": MLPClassifier()\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 001:  \n",
    "\n",
    "CSM: {'activation': 0, 'alpha': 9.190404654691428e-05, 'layer_size': 73.0, 'learning_rate': 0.0017325216661984056}\n",
    "\n",
    " DOC: {'activation': 2, 'alpha': 0.0002616927777929669, 'layer_size': 70.0, 'learning_rate': 0.0008885558916449781}\n",
    "\n",
    " MTS: {'activation': 2, 'alpha': 1.0637827193154231e-05, 'layer_size': 45.0, 'learning_rate': 0.025594709065934664}\n",
    "\n",
    " BCR: {'activation': 0, 'alpha': 0.005821356388227255, 'layer_size': 34.0, 'learning_rate': 0.0002702352915791481}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": MLPClassifier(hidden_layer_sizes=(73,),\n",
    "                        alpha=9.19, tol=1e-4, \n",
    "                        random_state=1, activation='logistic', \n",
    "                        learning_rate_init=0.001\n",
    "                       ),\n",
    "    \"death_from_other_causes\": MLPClassifier(hidden_layer_sizes=(70,),\n",
    "                        alpha=0.00026, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.009\n",
    "                       ),\n",
    "    \"mts\": MLPClassifier(hidden_layer_sizes=(45,),\n",
    "                        alpha=1.064, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.025\n",
    "                       ),\n",
    "    \"bcr\": MLPClassifier(hidden_layer_sizes=(34,),\n",
    "                        alpha=0.0058, tol=1e-4, \n",
    "                        random_state=1, activation='logistic', \n",
    "                        learning_rate_init=0.002\n",
    "                       )\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 001')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 002:  \n",
    "\n",
    "CSM: {'activation': 0, 'alpha': 9.190404654691428e-05, 'layer_size': 73.0, 'learning_rate': 0.0017325216661984056}\n",
    "\n",
    " DOC: {'activation': 1, 'alpha': 1.1772993907587444e-05, 'hidden_layer_sizes': 4, 'learning_rate': 1, 'max_iter': 0, 'solver': 2}\n",
    "\n",
    " MTS: {'activation': 1, 'alpha': 0.02629177201546902, 'hidden_layer_sizes': 4, 'learning_rate': 1, 'max_iter': 0, 'solver': 2}\n",
    "\n",
    " BCR: {'activation': 0, 'alpha': 0.0004952777642313747, 'hidden_layer_sizes': 1, 'learning_rate': 0, 'max_iter': 0, 'solver': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_column: cancer_specific_mortality\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: death_from_other_causes\n",
      "\tTrainable month: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: mts\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: bcr\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": MLPClassifier(hidden_layer_sizes=(73,),\n",
    "                        alpha=9.19, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.001\n",
    "                       ),\n",
    "    \"death_from_other_causes\": MLPClassifier(hidden_layer_sizes=(8,8,),\n",
    "                        alpha=1.1772993907587444e-05, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate='adaptive', solver='lbfgs', max_iter=3000\n",
    "                       ),\n",
    "    \"mts\": MLPClassifier(hidden_layer_sizes=(8,8,),\n",
    "                        alpha=0.02629177201546902, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate='adaptive', solver='lbfgs', max_iter=3000\n",
    "                       ),\n",
    "    \"bcr\": MLPClassifier(hidden_layer_sizes=(16,),\n",
    "                        alpha=0.0004952777642313747, tol=1e-4, \n",
    "                        random_state=1, activation='tanh', \n",
    "                        learning_rate='constant', solver='adam'\n",
    "                       )\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 002')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 003:  \n",
    "\n",
    "CSM: {'activation': 2, 'alpha': 3.670399366093249e-06, 'hidden_layer_sizes': 4, 'learning_rate': 1, 'max_iter': 0, 'solver': 2}\n",
    "\n",
    " DOC: {'activation': 2, 'alpha': 0.0002616927777929669, 'layer_size': 70.0, 'learning_rate': 0.0008885558916449781}\n",
    "\n",
    " MTS: {'activation': 2, 'alpha': 1.0637827193154231e-05, 'layer_size': 45.0, 'learning_rate': 0.025594709065934664}\n",
    "\n",
    " BCR: {'activation': 0, 'alpha': 0.005821356388227255, 'layer_size': 34.0, 'learning_rate': 0.0002702352915791481}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": MLPClassifier(hidden_layer_sizes=(16,16),\n",
    "                        alpha=3.670399366093249e-06, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate='adaptive', max_iter=500,\n",
    "                        solver='lbfgs'\n",
    "                       ),\n",
    "    \"death_from_other_causes\": MLPClassifier(hidden_layer_sizes=(70,),\n",
    "                        alpha=0.00026, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.009\n",
    "                       ),\n",
    "    \"mts\": MLPClassifier(hidden_layer_sizes=(45,),\n",
    "                        alpha=1.064, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.025\n",
    "                       ),\n",
    "    \"bcr\": MLPClassifier(hidden_layer_sizes=(34,),\n",
    "                        alpha=0.0058, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.002\n",
    "                       )\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 003')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 004:  \n",
    "\n",
    "CSM: {'activation': 1, 'alpha': 0.0002340750505402387, 'hidden_layer_sizes': 2, 'learning_rate': 0, 'max_iter': 0, 'solver': 1}\n",
    "\n",
    " DOC: {'activation': 2, 'alpha': 0.0002616927777929669, 'layer_size': 70.0, 'learning_rate': 0.0008885558916449781}\n",
    "\n",
    " MTS: {'activation': 2, 'alpha': 1.0637827193154231e-05, 'layer_size': 45.0, 'learning_rate': 0.025594709065934664}\n",
    "\n",
    " BCR: {'activation': 0, 'alpha': 0.005821356388227255, 'layer_size': 34.0, 'learning_rate': 0.0002702352915791481}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": MLPClassifier(hidden_layer_sizes=(32,),\n",
    "                        alpha=0.0002, tol=1e-4, \n",
    "                        random_state=1, activation='tanh', \n",
    "                        learning_rate='constant', max_iter=2000,\n",
    "                        solver='adam'\n",
    "                       ),\n",
    "    \"death_from_other_causes\": MLPClassifier(hidden_layer_sizes=(70,),\n",
    "                        alpha=0.00026, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.009\n",
    "                       ),\n",
    "    \"mts\": MLPClassifier(hidden_layer_sizes=(45,),\n",
    "                        alpha=1.064, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.025\n",
    "                       ),\n",
    "    \"bcr\": MLPClassifier(hidden_layer_sizes=(34,),\n",
    "                        alpha=0.0058, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.002\n",
    "                       )\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 004')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 005:  \n",
    "\n",
    "CSM: {'activation': 1, 'alpha': 6.835502772513517e-05, 'hidden_layer_sizes': 11, 'learning_rate': 0, 'max_iter': 0, 'solver': 1}\n",
    "\n",
    " DOC: {'activation': 2, 'alpha': 0.0002616927777929669, 'layer_size': 70.0, 'learning_rate': 0.0008885558916449781}\n",
    "\n",
    " MTS: {'activation': 2, 'alpha': 1.0637827193154231e-05, 'layer_size': 45.0, 'learning_rate': 0.025594709065934664}\n",
    "\n",
    " BCR: {'activation': 0, 'alpha': 0.005821356388227255, 'layer_size': 34.0, 'learning_rate': 0.0002702352915791481}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": MLPClassifier(hidden_layer_sizes=(32, 16, 16,),\n",
    "                        alpha=0, tol=1e-4, \n",
    "                        random_state=1, activation='tanh', \n",
    "                        learning_rate='constant', max_iter=2500,\n",
    "                        solver='adam'\n",
    "                       ),\n",
    "    \"death_from_other_causes\": MLPClassifier(hidden_layer_sizes=(70,),\n",
    "                        alpha=0.00026, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.009\n",
    "                       ),\n",
    "    \"mts\": MLPClassifier(hidden_layer_sizes=(45,),\n",
    "                        alpha=1.064, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.025\n",
    "                       ),\n",
    "    \"bcr\": MLPClassifier(hidden_layer_sizes=(34,),\n",
    "                        alpha=0.0058, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.002\n",
    "                       )\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 005')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 006:  \n",
    "\n",
    "CSM: {'activation': 1, 'alpha': 6.835502772513517e-05, 'hidden_layer_sizes': 11, 'learning_rate': 0, 'max_iter': 0, 'solver': 1}\n",
    "\n",
    " DOC: {'activation': 2, 'alpha': 0.0002616927777929669, 'layer_size': 70.0, 'learning_rate': 0.0008885558916449781}\n",
    "\n",
    " MTS: {'activation': 2, 'alpha': 1.0637827193154231e-05, 'layer_size': 45.0, 'learning_rate': 0.025594709065934664}\n",
    "\n",
    " BCR: {'activation': 0, 'alpha': 0.005821356388227255, 'layer_size': 34.0, 'learning_rate': 0.0002702352915791481}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_column: cancer_specific_mortality\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: death_from_other_causes\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: mts\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n",
      "target_column: bcr\n",
      "\tTrainable month: 24\n",
      "\tTrainable month: 36\n",
      "\tTrainable month: 48\n",
      "\tTrainable month: 60\n",
      "\tTrainable month: 72\n",
      "\tTrainable month: 84\n",
      "\tTrainable month: 96\n",
      "\tTrainable month: 108\n",
      "\tTrainable month: 120\n",
      "\tTrainable month: 132\n",
      "\tTrainable month: 144\n",
      "\tTrainable month: 156\n",
      "\tTrainable month: 168\n",
      "\tTrainable month: 180\n",
      "\tTrainable month: 192\n",
      "\tTrainable month: 204\n",
      "\tTrainable month: 216\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    \"cancer_specific_mortality\": MLPClassifier(hidden_layer_sizes=(256, 128, 64, 32, 32, 16),\n",
    "                        alpha=0, tol=1e-4, \n",
    "                        random_state=1, activation='tanh', \n",
    "                        learning_rate='constant', max_iter=2500,\n",
    "                        solver='adam'\n",
    "                       ),\n",
    "    \"death_from_other_causes\": MLPClassifier(hidden_layer_sizes=(70,),\n",
    "                        alpha=0.00026, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.009\n",
    "                       ),\n",
    "    \"mts\": MLPClassifier(hidden_layer_sizes=(45,),\n",
    "                        alpha=1.064, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.025\n",
    "                       ),\n",
    "    \"bcr\": MLPClassifier(hidden_layer_sizes=(34,),\n",
    "                        alpha=0.0058, tol=1e-4, \n",
    "                        random_state=1, activation='relu', \n",
    "                        learning_rate_init=0.002\n",
    "                       )\n",
    "}\n",
    "\n",
    "run_experiment_wrapper(data_train.copy(), data_test.copy(), model_dict, experiment_name='Experiment 006')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4313b30ba1dc00c95c03b070820057b6c164c878ae3e5e81f7647b57ebcaab7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
