{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook to test and find best hyperparameters for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 20364\n",
      "Number of CPUs in the system: 16\n",
      "executing command 'taskset -cp 0,1,2,3,4,5,6,7,8,9,10,11,12,13 20364' ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pid = os.getpid()\n",
    "print(\"PID: %i\" % pid)\n",
    "\n",
    "n_cpu = os.cpu_count()   # Number of CPUs assigned to this process\n",
    "print(\"Number of CPUs in the system:\", n_cpu)\n",
    "\n",
    "# we won't use all the available cpu's for this script \n",
    "n_jobs = n_cpu - 2 # The number of tasks to run in parallel\n",
    "\n",
    "# Control which CPUs are made available for this script\n",
    "cpu_arg = ''.join([str(ci) + ',' for ci in list(range(n_jobs))])[:-1]\n",
    "cmd = 'taskset -cp %s %i' % (cpu_arg, pid)\n",
    "print(\"executing command '%s' ...\" % cmd)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to cleaned data file\n",
    "trainFilePath = 'data/data_train.pkl'\n",
    "\n",
    "# Read already prepared and saved train datasets\n",
    "with open(trainFilePath, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "# Create dummy variables for categorical data\n",
    "data_train = pd.get_dummies(data_train, columns=['clinical_stage', 'biopsy_gleason_gg', 'pathological_gleason_gg',\n",
    "                                'pathologic_stage', 'lni', 'surgical_margin_status', 'persistent_psa',\n",
    "                                'TRYSgrupes', 'PLNDO1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explodes the provided \"df\" dataset based on provided survival column \"time\" and\n",
    "clips the data to be in a range [min_time; max_time] (). A new discrete survival column\n",
    "will be created with name set as variable \"time_discrete\". \"cum_event\" boolean determines\n",
    "if cumulative event column will be created or no.\n",
    "\n",
    "clip(lower, upper) function will help us create a new discrete survival time column. \n",
    "If we specify lower=1 and upper=200, patients who experienced event earlier than 200th \n",
    "month will only have records till their event, on other side, if a patient survived past \n",
    "200th month, we will clip this information and will only keep information about him til 200th month.\n",
    "Another example, if we specify lower=140 and upper=200, and if the person experienced event \n",
    "at 100th month, we will create records for him till 140th (lower boundary) month.\n",
    "\"\"\"\n",
    "def explode_data(df,max_time,time,target_column,min_time=1,\n",
    "                 time_discrete='survival_time_discrete',cum_event=False):\n",
    "\n",
    "    target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "    # We create a new time column and clip the data by provided min and max survival times\n",
    "    df[time_discrete] = df[time].clip(min_time,max_time).apply(range)\n",
    "\n",
    "    # Exploding the dataset with the created range value in new time column\n",
    "    data_exploded = df.explode(time_discrete)\n",
    "    data_exploded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # New column starts at 0, we'll increase each value by 1\n",
    "    data_exploded[time_discrete] = pd.to_numeric(data_exploded[time_discrete]) + 1\n",
    "\n",
    "    # New event column, which will indicate the last event date\n",
    "    data_exploded[target_column_discrete] = (data_exploded[time_discrete] >= data_exploded[time]) * pd.to_numeric(data_exploded[target_column])\n",
    "    \n",
    "    if cum_event == True:\n",
    "        target_column_cumulative = target_column + '_cumulative'\n",
    "\n",
    "        # Create new event column with duplicated event values from discrete column\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_discrete]\n",
    "        \n",
    "        # For cumulative events, after end_time we will have NA values, we'll replace those with event indicator\n",
    "        after_survival_time = data_exploded[time_discrete] > data_exploded[time]\n",
    "        data_exploded.loc[after_survival_time, target_column_discrete] = -1\n",
    "        data_exploded[target_column_discrete] = data_exploded[target_column_discrete].replace(-1,np.NaN)\n",
    "        data_exploded.loc[(after_survival_time & (data_exploded[target_column]==0)), target_column_cumulative] = -1\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_cumulative].replace(-1,np.NaN)\n",
    "\n",
    "    return data_exploded\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given an exploded dataset with instant mortality probabilities \"event_probability_column\"\n",
    "and \"id_column\" for grouping (optional), cumulative hazard column will be calculated\n",
    "\"\"\"\n",
    "def cumulative_hazard(df, event_probability_column, id_column):\n",
    "    data_copy = df.copy()\n",
    "    if id_column is not None:\n",
    "        data_copy = data_copy[ [id_column, event_probability_column] ]\n",
    "    else:\n",
    "        data_copy = data_copy[ [event_probability_column] ]\n",
    "    data_copy['negative_log_prob'] = np.log( 1 - data_copy[event_probability_column] )\n",
    "    if id_column is not None:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy.groupby(id_column)['negative_log_prob'].transform(pd.Series.cumsum))\n",
    "    else:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy['negative_log_prob'].cumsum())\n",
    "    return data_copy['cumulative_hazard']\n",
    "\n",
    "\n",
    "\n",
    "def add_predict_probabilities_optimized(df_exploded, target_column, model):\n",
    "    \"\"\"\n",
    "    Given exploded datase. Adds predictend instant mortality probabilities as well as cumulative ones.\n",
    "    \"\"\"\n",
    "\n",
    "    df_exploded_copy = df_exploded.copy()\n",
    "    df_exploded_copy.drop(['patient_id', target_column + '_cumulative'], axis=1, inplace=True)\n",
    "    \n",
    "    # probabilities\n",
    "    y_pred = model.predict_proba(df_exploded_copy)[:,1]\n",
    "    df_exploded['mortality_instant_prob'] = y_pred\n",
    "\n",
    "    # Cumulative hazard for each patient\n",
    "    df_exploded['cumulative_hazard'] = cumulative_hazard(df_exploded,'mortality_instant_prob','patient_id')\n",
    "    \n",
    "    return df_exploded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 200\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>psa</th>\n",
       "      <th>biopsy_gleason</th>\n",
       "      <th>pathologic_gleason</th>\n",
       "      <th>bcr</th>\n",
       "      <th>mts</th>\n",
       "      <th>death_from_other_causes</th>\n",
       "      <th>clinical_stage_1</th>\n",
       "      <th>clinical_stage_2</th>\n",
       "      <th>clinical_stage_3</th>\n",
       "      <th>...</th>\n",
       "      <th>surgical_margin_status_0</th>\n",
       "      <th>surgical_margin_status_1</th>\n",
       "      <th>persistent_psa_0</th>\n",
       "      <th>persistent_psa_1</th>\n",
       "      <th>TRYSgrupes_0</th>\n",
       "      <th>TRYSgrupes_1</th>\n",
       "      <th>TRYSgrupes_2</th>\n",
       "      <th>PLNDO1_0</th>\n",
       "      <th>PLNDO1_1</th>\n",
       "      <th>survival_time_discrete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.0</td>\n",
       "      <td>6.36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250195</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250196</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250197</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250198</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250199</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250200 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age   psa  biopsy_gleason  pathologic_gleason bcr mts  \\\n",
       "0       73.0  6.36             6.0                 7.0   1   0   \n",
       "1       73.0  6.36             6.0                 7.0   1   0   \n",
       "2       73.0  6.36             6.0                 7.0   1   0   \n",
       "3       73.0  6.36             6.0                 7.0   1   0   \n",
       "4       73.0  6.36             6.0                 7.0   1   0   \n",
       "...      ...   ...             ...                 ...  ..  ..   \n",
       "250195  68.0  6.00             6.0                 8.0   1   0   \n",
       "250196  68.0  6.00             6.0                 8.0   1   0   \n",
       "250197  68.0  6.00             6.0                 8.0   1   0   \n",
       "250198  68.0  6.00             6.0                 8.0   1   0   \n",
       "250199  68.0  6.00             6.0                 8.0   1   0   \n",
       "\n",
       "       death_from_other_causes  clinical_stage_1  clinical_stage_2  \\\n",
       "0                            0                 0                 0   \n",
       "1                            0                 0                 0   \n",
       "2                            0                 0                 0   \n",
       "3                            0                 0                 0   \n",
       "4                            0                 0                 0   \n",
       "...                        ...               ...               ...   \n",
       "250195                       0                 0                 1   \n",
       "250196                       0                 0                 1   \n",
       "250197                       0                 0                 1   \n",
       "250198                       0                 0                 1   \n",
       "250199                       0                 0                 1   \n",
       "\n",
       "        clinical_stage_3  ...  surgical_margin_status_0  \\\n",
       "0                      1  ...                         0   \n",
       "1                      1  ...                         0   \n",
       "2                      1  ...                         0   \n",
       "3                      1  ...                         0   \n",
       "4                      1  ...                         0   \n",
       "...                  ...  ...                       ...   \n",
       "250195                 0  ...                         0   \n",
       "250196                 0  ...                         0   \n",
       "250197                 0  ...                         0   \n",
       "250198                 0  ...                         0   \n",
       "250199                 0  ...                         0   \n",
       "\n",
       "        surgical_margin_status_1  persistent_psa_0  persistent_psa_1  \\\n",
       "0                              1                 1                 0   \n",
       "1                              1                 1                 0   \n",
       "2                              1                 1                 0   \n",
       "3                              1                 1                 0   \n",
       "4                              1                 1                 0   \n",
       "...                          ...               ...               ...   \n",
       "250195                         1                 1                 0   \n",
       "250196                         1                 1                 0   \n",
       "250197                         1                 1                 0   \n",
       "250198                         1                 1                 0   \n",
       "250199                         1                 1                 0   \n",
       "\n",
       "        TRYSgrupes_0  TRYSgrupes_1  TRYSgrupes_2  PLNDO1_0  PLNDO1_1  \\\n",
       "0                  0             1             0         1         0   \n",
       "1                  0             1             0         1         0   \n",
       "2                  0             1             0         1         0   \n",
       "3                  0             1             0         1         0   \n",
       "4                  0             1             0         1         0   \n",
       "...              ...           ...           ...       ...       ...   \n",
       "250195             0             0             1         1         0   \n",
       "250196             0             0             1         1         0   \n",
       "250197             0             0             1         1         0   \n",
       "250198             0             0             1         1         0   \n",
       "250199             0             0             1         1         0   \n",
       "\n",
       "        survival_time_discrete  \n",
       "0                            1  \n",
       "1                            2  \n",
       "2                            3  \n",
       "3                            4  \n",
       "4                            5  \n",
       "...                        ...  \n",
       "250195                     196  \n",
       "250196                     197  \n",
       "250197                     198  \n",
       "250198                     199  \n",
       "250199                     200  \n",
       "\n",
       "[250200 rows x 36 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators':np.arange(150,700,50),\n",
    "              'max_features':np.arange(0.1, 1, 0.1),\n",
    "              'max_depth': [3, 5, 7, 9],\n",
    "              'max_samples': [0.3, 0.5, 0.8],\n",
    "              'min_samples_leaf': [1, 2, 3, 4, 5]}\n",
    "\n",
    "rf_random = RSCV(RandomForestClassifier(), param_grid, n_iter=15, \n",
    "             random_state=0, verbose=2, scoring='roc_auc', n_jobs=n_jobs)\n",
    "model_rf = rf_random.fit(X_train, y_train)\n",
    "model_rf_best = model_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_leaf': 5,\n",
       " 'max_samples': 0.3,\n",
       " 'max_features': 0.5,\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "model_rf.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancer specific mortality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [14:09<00:00, 10.61s/trial, best loss: -0.9522071189577052]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 19.560721062307877,\n",
       " 'max_features': 2,\n",
       " 'min_samples_leaf': 0.0008970113318042469,\n",
       " 'min_samples_split': 0.03442949793468529,\n",
       " 'n_estimators': 117.17628335911297}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 200.0,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 0.043813847274908446,\n",
       " 'min_samples_split': 0.551636834827235,\n",
       " 'n_estimators': 5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:15<00:00,  6.95s/trial, best loss: -0.9026579108637769]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 0,\n",
       " 'max_depth': 17.937192102303005,\n",
       " 'max_features': 2,\n",
       " 'min_samples_leaf': 0.03932576931296611,\n",
       " 'min_samples_split': 0.025730604369255006,\n",
       " 'n_estimators': 494.2199824963416}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### death from other causes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'death_from_other_causes'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:08<00:00,  6.86s/trial, best loss: -0.8098503112132167]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 9.859569892589294,\n",
       " 'max_features': 0,\n",
       " 'min_samples_leaf': 0.004777233684864247,\n",
       " 'min_samples_split': 0.04259570641528535,\n",
       " 'n_estimators': 187.7291402134495}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'death_from_other_causes'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [07:58<00:00,  5.98s/trial, best loss: -0.6062502734573373]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 0,\n",
       " 'max_depth': 15.99874407715512,\n",
       " 'max_features': 2,\n",
       " 'min_samples_leaf': 0.05467350337454149,\n",
       " 'min_samples_split': 0.5886741613999928,\n",
       " 'n_estimators': 376.7957625705628}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'mts'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [07:33<00:00,  5.67s/trial, best loss: -0.9064147406361287]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 0,\n",
       " 'max_depth': 6.48444416018883,\n",
       " 'max_features': 0,\n",
       " 'min_samples_leaf': 0.01920890664943756,\n",
       " 'min_samples_split': 0.0859146871809218,\n",
       " 'n_estimators': 450.3159883151506}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'mts'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [03:49<00:00,  2.87s/trial, best loss: -0.9636169531441549]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 13.577996616687528,\n",
       " 'max_features': 1,\n",
       " 'min_samples_leaf': 0.036805117857165615,\n",
       " 'min_samples_split': 0.3255535986892048,\n",
       " 'n_estimators': 149.6122350278912}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bcr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'bcr'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [09:35<00:00,  7.19s/trial, best loss: -0.9324703081729039]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 0,\n",
       " 'max_depth': 18.542798032669822,\n",
       " 'max_features': 0,\n",
       " 'min_samples_leaf': 0.0011043494935267933,\n",
       " 'min_samples_split': 0.000671643460114342,\n",
       " 'n_estimators': 316.6923084008164}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'bcr'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [03:36<00:00,  2.70s/trial, best loss: -0.8865986500383168]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 9.362170539012933,\n",
       " 'max_features': 1,\n",
       " 'min_samples_leaf': 0.04889852261499672,\n",
       " 'min_samples_split': 0.2490075040108427,\n",
       " 'n_estimators': 160.7620972043769}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancer specific mortality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [06:28<00:00,  9.72s/trial, best loss: -0.9987603535217533]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9,\n",
       " 'gamma': 0.0,\n",
       " 'learning_rate': 0.2,\n",
       " 'max_depth': 18.0,\n",
       " 'min_child_weight': 2.0,\n",
       " 'n_estimators': 494.0763574289686,\n",
       " 'reg_alpha': 0.011624533771820083,\n",
       " 'reg_lambda': 2.3750241072036045,\n",
       " 'scale_pos_weight': 0.884642762544353,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 40, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [08:38<00:00,  6.48s/trial, best loss: -0.8883445945029762]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9,\n",
       " 'gamma': 0.4,\n",
       " 'learning_rate': 0.05,\n",
       " 'max_depth': 10.0,\n",
       " 'min_child_weight': 8.0,\n",
       " 'n_estimators': 691.0814977776121,\n",
       " 'reg_alpha': 0.036934916140565495,\n",
       " 'reg_lambda': 4.266113036560311,\n",
       " 'scale_pos_weight': 0.9921922974444556,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### death from other causes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'death_from_other_causes'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:43<00:00, 16.09s/trial, best loss: -0.9992182587944143]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.65,\n",
       " 'gamma': 0.25,\n",
       " 'learning_rate': 0.17500000000000002,\n",
       " 'max_depth': 11.0,\n",
       " 'min_child_weight': 1.0,\n",
       " 'n_estimators': 618.9569732240903,\n",
       " 'reg_alpha': 0.00460840736862405,\n",
       " 'reg_lambda': 1.5957720013269137,\n",
       " 'scale_pos_weight': 0.9929006236241394,\n",
       " 'subsample': 0.7000000000000001}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 40, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'death_from_other_causes'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [14:56<00:00, 11.21s/trial, best loss: -0.6426574234416307]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8500000000000001,\n",
       " 'gamma': 0.1,\n",
       " 'learning_rate': 0.025,\n",
       " 'max_depth': 13.0,\n",
       " 'min_child_weight': 1.0,\n",
       " 'n_estimators': 729.6590585991698,\n",
       " 'reg_alpha': 0.030777819762639377,\n",
       " 'reg_lambda': 7.139170115130897,\n",
       " 'scale_pos_weight': 0.29417941021561633,\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'mts'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [05:37<00:00,  8.45s/trial, best loss: -0.9946783058491052]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.55,\n",
       " 'gamma': 0.05,\n",
       " 'learning_rate': 0.15000000000000002,\n",
       " 'max_depth': 13.0,\n",
       " 'min_child_weight': 2.0,\n",
       " 'n_estimators': 535.373615733675,\n",
       " 'reg_alpha': 0.012615056264249302,\n",
       " 'reg_lambda': 0.7584283803105774,\n",
       " 'scale_pos_weight': 0.9095133867383003,\n",
       " 'subsample': 0.65}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 40, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'mts'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [06:19<00:00,  4.74s/trial, best loss: -0.9663154025765888]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9,\n",
       " 'gamma': 0.35000000000000003,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 7.0,\n",
       " 'min_child_weight': 1.0,\n",
       " 'n_estimators': 666.5875643268608,\n",
       " 'reg_alpha': 0.07425918532653869,\n",
       " 'reg_lambda': 7.139570109761258,\n",
       " 'scale_pos_weight': 0.19101970999175796,\n",
       " 'subsample': 0.9500000000000001}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bcr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### only train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'bcr'\n",
    "max_time = 216\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_train_copy_exploded_cumulative = explode_data(data_train.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_train_copy_exploded_cumulative = df_train_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_train_copy_exploded_pred = add_predict_probabilities_optimized(df_train_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_train_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [06:04<00:00,  9.12s/trial, best loss: -0.9833019511876315]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.7000000000000001,\n",
       " 'gamma': 0.1,\n",
       " 'learning_rate': 0.2,\n",
       " 'max_depth': 19.0,\n",
       " 'min_child_weight': 1.0,\n",
       " 'n_estimators': 320.2127746661102,\n",
       " 'reg_alpha': 0.06919873757682321,\n",
       " 'reg_lambda': 0.01593110711960384,\n",
       " 'scale_pos_weight': 0.4838101096249437,\n",
       " 'subsample': 0.7000000000000001}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 40, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'bcr'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "    \n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [05:54<00:00,  4.44s/trial, best loss: -0.8920886403727195]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8500000000000001,\n",
       " 'gamma': 0.7000000000000001,\n",
       " 'learning_rate': 0.125,\n",
       " 'max_depth': 2.0,\n",
       " 'min_child_weight': 10.0,\n",
       " 'n_estimators': 563.9364756838096,\n",
       " 'reg_alpha': 0.09983298911625862,\n",
       " 'reg_lambda': 8.476381530931778,\n",
       " 'scale_pos_weight': 0.17369127320043823,\n",
       " 'subsample': 0.75}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 216\n",
    "\n",
    "data_train_copy = data_train.copy()\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "X = data_train_copy.drop(target_column, axis=1)\n",
    "y = data_train_copy[target_column]\n",
    "\n",
    "\n",
    "# 90/10 split and stratify based on 'overall_mortality'\n",
    "data_train_hyp, data_val_hyp, y_train_hyp, y_test_hyp = train_test_split(\n",
    "    data_train_copy,\n",
    "    data_train_copy[target_columns], test_size=0.1, random_state=2, \n",
    "    stratify=data_train_copy[target_column])\n",
    "\n",
    "data_train_hyp[target_columns] = y_train_hyp\n",
    "data_val_hyp[target_columns] = y_test_hyp\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]\n",
    "\n",
    "\n",
    "# Explode the dataset\n",
    "df_val_copy_exploded = explode_data(data_val_hyp.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_val = df_val_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_val = df_val_copy_exploded[target_column_discrete]\n",
    "\n",
    "df_val_copy_exploded_cumulative = explode_data(data_val_hyp.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "x_columns_to_drop_exploded_cumulative = [target_column+'_discrete', 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "x_columns_to_drop_exploded_cumulative.extend(target_columns)\n",
    "df_val_copy_exploded_cumulative = df_val_copy_exploded_cumulative.drop(x_columns_to_drop_exploded_cumulative, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(space):\n",
    "    \n",
    "    # Create StratifiedKFold object.\n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "    for num, (train_id, valid_id) in enumerate(skf.split(X, y)):\n",
    "        X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n",
    "        y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n",
    "        \n",
    "        \n",
    "\n",
    "        model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                      colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                      max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                     min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                      reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                verbose = False,\n",
    "                eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                eval_metric = 'auc',\n",
    "                early_stopping_rounds = 250)\n",
    "        \n",
    "        #Mean of the predictions\n",
    "        preds += model.predict(X_test) / 10 # Splits\n",
    "        \n",
    "        #Mean of feature importance\n",
    "        model_fi += model.feature_importances_ / 10 #splits\n",
    "        \n",
    "        #Out of Fold predictions\n",
    "        oof_preds[valid_id] = model.predict(X_valid)\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n",
    "        print(f\"Fold {num} | RMSE: {fold_rmse}\")\n",
    "        \n",
    "        mean_rmse += fold_rmse / 10\n",
    "\n",
    "    # fit the data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # add predicted probabilities\n",
    "    df_val_copy_exploded_pred = add_predict_probabilities_optimized(df_val_copy_exploded_cumulative.copy(), target_column, model)\n",
    "    \n",
    "    # calculate auc\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Validation data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_val_copy_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_copy_exploded_pred[target_column+'_cumulative'])\n",
    "        sub_dat = df_val_copy_exploded_pred[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 216\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "data_train_copy = data_train.copy().reset_index(drop=True)\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "def objective(space):\n",
    "\n",
    "    # Create StratifiedKFold object.\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    auc_cv = []\n",
    "\n",
    "    for train_index, val_index in skf.split(data_train_copy, data_train_copy.loc[:,target_column]):\n",
    "        \n",
    "        # get train and validation sets\n",
    "        train = data_train_copy.loc[train_index,:]\n",
    "        val = data_train_copy.loc[val_index,:]\n",
    "        \n",
    "        # explode train set\n",
    "        train_exploded = explode_data(train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "        # explode validation set with cumulative events\n",
    "        val_exploded_cumulative = explode_data(val.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "\n",
    "        # define model\n",
    "        model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "        \n",
    "        # columns to drop before fitting\n",
    "        x_columns_to_drop = [target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "        x_columns_to_drop.extend(target_columns)\n",
    "\n",
    "        # train X and y\n",
    "        X_train = train_exploded.drop(x_columns_to_drop, axis=1)\n",
    "        y_train = train_exploded[target_column_discrete]\n",
    "\n",
    "        # fit model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # columns to drop before making predictions\n",
    "        x_columns_to_drop = [target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "        x_columns_to_drop.extend(target_columns)\n",
    "\n",
    "        # add predicted probabilities\n",
    "        df_val_exploded_pred = add_predict_probabilities_optimized(val_exploded_cumulative.drop(x_columns_to_drop,axis=1), target_column, model)\n",
    "\n",
    "        # calculate auc\n",
    "        # AUC for each cumulative slice\n",
    "        # Months at which we'll check the AUC's\n",
    "        months = list(range(6, max_time, 6))\n",
    "\n",
    "        val_auc_stats = []\n",
    "        for month in months:\n",
    "            # --- Validation data ---\n",
    "            # Selecting a subset of data based on the months\n",
    "            select = (df_val_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_exploded_pred[target_column+'_cumulative'])\n",
    "            sub_dat = df_val_exploded_pred[select]\n",
    "\n",
    "            # If in the sliced data there's a event, calculate AUC metric,\n",
    "            # otherwise assign NaN value\n",
    "            if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "                fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "                auc_stat = auc(fpr, tpr)\n",
    "            else:\n",
    "                auc_stat = float('NaN')\n",
    "            val_auc_stats.append(auc_stat)\n",
    "\n",
    "        auc_mean = np.nanmean(val_auc_stats)\n",
    "\n",
    "        auc_cv.append(auc_mean)\n",
    "\n",
    "    auc_cv_mean = np.nanmean(auc_cv)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_cv_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [38:23<00:00, 28.79s/trial, best loss: -0.8941533300609423]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 0,\n",
       " 'max_depth': 5.008074817422085,\n",
       " 'max_features': 1,\n",
       " 'min_samples_leaf': 0.0348778696429207,\n",
       " 'min_samples_split': 0.10209281579299412,\n",
       " 'n_estimators': 334.0017140265728}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 80, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
