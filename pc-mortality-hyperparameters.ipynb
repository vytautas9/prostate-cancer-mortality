{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook to test and find best hyperparameters for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 26064\n",
      "Number of CPUs in the system: 16\n",
      "executing command 'taskset -cp 0,1,2,3,4,5,6,7,8,9,10,11,12,13 26064' ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pid = os.getpid()\n",
    "print(\"PID: %i\" % pid)\n",
    "\n",
    "n_cpu = os.cpu_count()   # Number of CPUs assigned to this process\n",
    "print(\"Number of CPUs in the system:\", n_cpu)\n",
    "\n",
    "# we won't use all the available cpu's for this script \n",
    "n_jobs = n_cpu - 2 # The number of tasks to run in parallel\n",
    "\n",
    "# Control which CPUs are made available for this script\n",
    "cpu_arg = ''.join([str(ci) + ',' for ci in list(range(n_jobs))])[:-1]\n",
    "cmd = 'taskset -cp %s %i' % (cpu_arg, pid)\n",
    "print(\"executing command '%s' ...\" % cmd)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to cleaned data file\n",
    "trainFilePath = 'data/data_train.pkl'\n",
    "\n",
    "# Read already prepared and saved train datasets\n",
    "with open(trainFilePath, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "# Create dummy variables for categorical data\n",
    "data_train = pd.get_dummies(data_train, columns=['clinical_stage', 'biopsy_gleason_gg', 'pathological_gleason_gg',\n",
    "                                'pathologic_stage', 'lni', 'surgical_margin_status', 'persistent_psa',\n",
    "                                'TRYSgrupes', 'PLNDO1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explodes the provided \"df\" dataset based on provided survival column \"time\" and\n",
    "clips the data to be in a range [min_time; max_time] (). A new discrete survival column\n",
    "will be created with name set as variable \"time_discrete\". \"cum_event\" boolean determines\n",
    "if cumulative event column will be created or no.\n",
    "\n",
    "clip(lower, upper) function will help us create a new discrete survival time column. \n",
    "If we specify lower=1 and upper=200, patients who experienced event earlier than 200th \n",
    "month will only have records till their event, on other side, if a patient survived past \n",
    "200th month, we will clip this information and will only keep information about him til 200th month.\n",
    "Another example, if we specify lower=140 and upper=200, and if the person experienced event \n",
    "at 100th month, we will create records for him till 140th (lower boundary) month.\n",
    "\"\"\"\n",
    "def explode_data(df,max_time,time,target_column,min_time=1,\n",
    "                 time_discrete='survival_time_discrete',cum_event=False):\n",
    "\n",
    "    target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "    # We create a new time column and clip the data by provided min and max survival times\n",
    "    df[time_discrete] = df[time].clip(min_time,max_time).apply(range)\n",
    "\n",
    "    # Exploding the dataset with the created range value in new time column\n",
    "    data_exploded = df.explode(time_discrete)\n",
    "    data_exploded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # New column starts at 0, we'll increase each value by 1\n",
    "    data_exploded[time_discrete] = pd.to_numeric(data_exploded[time_discrete]) + 1\n",
    "\n",
    "    # New event column, which will indicate the last event date\n",
    "    data_exploded[target_column_discrete] = (data_exploded[time_discrete] >= data_exploded[time]) * pd.to_numeric(data_exploded[target_column])\n",
    "    \n",
    "    if cum_event == True:\n",
    "        target_column_cumulative = target_column + '_cumulative'\n",
    "\n",
    "        # Create new event column with duplicated event values from discrete column\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_discrete]\n",
    "        \n",
    "        # For cumulative events, after end_time we will have NA values, we'll replace those with event indicator\n",
    "        after_survival_time = data_exploded[time_discrete] > data_exploded[time]\n",
    "        data_exploded.loc[after_survival_time, target_column_discrete] = -1\n",
    "        data_exploded[target_column_discrete] = data_exploded[target_column_discrete].replace(-1,np.NaN)\n",
    "        data_exploded.loc[(after_survival_time & (data_exploded[target_column]==0)), target_column_cumulative] = -1\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_cumulative].replace(-1,np.NaN)\n",
    "\n",
    "    return data_exploded\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given an exploded dataset with instant mortality probabilities \"event_probability_column\"\n",
    "and \"id_column\" for grouping (optional), cumulative hazard column will be calculated\n",
    "\"\"\"\n",
    "def cumulative_hazard(df, event_probability_column, id_column):\n",
    "    data_copy = df.copy()\n",
    "    if id_column is not None:\n",
    "        data_copy = data_copy[ [id_column, event_probability_column] ]\n",
    "    else:\n",
    "        data_copy = data_copy[ [event_probability_column] ]\n",
    "    data_copy['negative_log_prob'] = np.log( 1 - data_copy[event_probability_column] )\n",
    "    if id_column is not None:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy.groupby(id_column)['negative_log_prob'].transform(pd.Series.cumsum))\n",
    "    else:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy['negative_log_prob'].cumsum())\n",
    "    return data_copy['cumulative_hazard']\n",
    "\n",
    "\n",
    "\n",
    "def add_predict_probabilities_optimized(df_exploded, target_column, model):\n",
    "    \"\"\"\n",
    "    Given exploded datase. Adds predictend instant mortality probabilities as well as cumulative ones.\n",
    "    \"\"\"\n",
    "\n",
    "    df_exploded_copy = df_exploded.copy()\n",
    "    df_exploded_copy.drop(['patient_id', target_column + '_cumulative'], axis=1, inplace=True)\n",
    "    \n",
    "    # probabilities\n",
    "    y_pred = model.predict_proba(df_exploded_copy)[:,1]\n",
    "    df_exploded['mortality_instant_prob'] = y_pred\n",
    "\n",
    "    # Cumulative hazard for each patient\n",
    "    df_exploded['cumulative_hazard'] = cumulative_hazard(df_exploded,'mortality_instant_prob','patient_id')\n",
    "    \n",
    "    return df_exploded\n",
    "\n",
    "\n",
    "def optimize_model_stratifiedkfold(target_column, max_time, space, df, model_type, max_evals):\n",
    "    data_train_copy = df.copy().reset_index(drop=True)\n",
    "\n",
    "    # mts and bcr have different survival months columns\n",
    "    match target_column:\n",
    "        case 'mts':\n",
    "            time = 'survival_months_mts'\n",
    "        case 'bcr':\n",
    "            time = 'survival_months_bcr'\n",
    "        case _:\n",
    "            time = 'survival_months'\n",
    "\n",
    "    target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "    # List of columns names which will be dropped from feature set before fitting the model\n",
    "    target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "    def objective(space):\n",
    "\n",
    "        # Create StratifiedKFold object.\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "        auc_cv = []\n",
    "\n",
    "        for train_index, val_index in skf.split(data_train_copy, data_train_copy.loc[:,target_column]):\n",
    "            \n",
    "            # get train and validation sets\n",
    "            train = data_train_copy.loc[train_index,:]\n",
    "            val = data_train_copy.loc[val_index,:]\n",
    "            \n",
    "            # explode train set\n",
    "            train_exploded = explode_data(train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "            # explode validation set with cumulative events\n",
    "            val_exploded_cumulative = explode_data(val.copy(), max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "\n",
    "            # define model\n",
    "            match model_type:\n",
    "                case 'RF':\n",
    "                    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "                case 'XGBoost':\n",
    "                    model = XGBClassifier(objective=\"binary:logistic\", random_state=0, booster='gbtree', eval_metric='auc',\n",
    "                        colsample_bytree=space['colsample_bytree'], gamma=space['gamma'], learning_rate=space['learning_rate'],\n",
    "                        max_depth=int(space['max_depth']), n_estimators=int(space['n_estimators']), subsample=space['subsample'],\n",
    "                        min_child_weight=space['min_child_weight'], scale_pos_weight=space['scale_pos_weight'], \n",
    "                        reg_lambda=space['reg_lamda'], reg_alpha=space['reg_alpha'])\n",
    "                case 'NN':\n",
    "                    model = MLPClassifier(hidden_layer_sizes=space['hidden_layer_sizes'],\n",
    "                        alpha=space['alpha'], tol=1e-4, \n",
    "                        random_state=1, activation=space['activation'], \n",
    "                        learning_rate=space['learning_rate'],\n",
    "                        max_iter=space['max_iter'],\n",
    "                        solver=space['solver']\n",
    "                       )\n",
    "                case _:\n",
    "                    pass\n",
    "            \n",
    "            # columns to drop before fitting\n",
    "            x_columns_to_drop = [target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "            x_columns_to_drop.extend(target_columns)\n",
    "\n",
    "            # train X and y\n",
    "            X_train = train_exploded.drop(x_columns_to_drop, axis=1)\n",
    "            y_train = train_exploded[target_column_discrete]\n",
    "\n",
    "            # fit model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # columns to drop before making predictions\n",
    "            x_columns_to_drop = [target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts']\n",
    "            x_columns_to_drop.extend(target_columns)\n",
    "\n",
    "            # add predicted probabilities\n",
    "            df_val_exploded_pred = add_predict_probabilities_optimized(val_exploded_cumulative.drop(x_columns_to_drop,axis=1), target_column, model)\n",
    "\n",
    "            # calculate auc\n",
    "            # AUC for each cumulative slice\n",
    "            # Months at which we'll check the AUC's\n",
    "            months = list(range(6, max_time, 6))\n",
    "\n",
    "            val_auc_stats = []\n",
    "            for month in months:\n",
    "                # --- Validation data ---\n",
    "                # Selecting a subset of data based on the months\n",
    "                select = (df_val_exploded_pred['survival_time_discrete'] == month) & pd.notna(df_val_exploded_pred[target_column+'_cumulative'])\n",
    "                sub_dat = df_val_exploded_pred[select]\n",
    "\n",
    "                # If in the sliced data there's a event, calculate AUC metric,\n",
    "                # otherwise assign NaN value\n",
    "                if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "                    fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "                    auc_stat = auc(fpr, tpr)\n",
    "                else:\n",
    "                    auc_stat = float('NaN')\n",
    "                val_auc_stats.append(auc_stat)\n",
    "\n",
    "            auc_mean = np.nanmean(val_auc_stats)\n",
    "\n",
    "            auc_cv.append(auc_mean)\n",
    "\n",
    "        auc_cv_mean = np.nanmean(auc_cv)\n",
    "\n",
    "        # We aim to maximize auc, therefore we return it as a negative value\n",
    "        return {'loss': -auc_cv_mean, 'status': STATUS_OK }\n",
    "    \n",
    "    trials = Trials()\n",
    "    best = fmin(fn = objective,\n",
    "                space = space,\n",
    "                algo = tpe.suggest,\n",
    "                max_evals = max_evals,\n",
    "                trials = trials)\n",
    "    \n",
    "    print(best)\n",
    "    \n",
    "    return trials, best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [28:19<00:00, 21.25s/trial, best loss: -0.8930259526960415]\n",
      "{'criterion': 1, 'max_depth': 13.574069212358445, 'max_features': 0, 'min_samples_leaf': 0.03168091798222092, 'min_samples_split': 0.4308128762421261, 'n_estimators': 308.1562762776964}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='cancer_specific_mortality',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='RF',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### death from other causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_model = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='death_from_other_causes',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='RF',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_model = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='mts',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='RF',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_model = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='bcr',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='RF',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_model = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='cancer_specific_mortality',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='XGBoost',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### death from other causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_model = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='death_from_other_causes',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='XGBoost',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_model = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='mts',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='XGBoost',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_model = {\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "    'gamma': hp.quniform('gamma', 0, 1, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0, 0.2, 0.025),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 20, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 0.1),\n",
    "    'reg_lamda': hp.uniform('reg_lambda', 0, 10),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0, 1)\n",
    "}\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='bcr',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='XGBoost',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial neural network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [51:17<00:00, 38.47s/trial, best loss: -0.8689690596625324] \n",
      "{'activation': 0, 'alpha': 9.190404654691428e-05, 'layer_size': 73.0, 'learning_rate': 0.0017325216661984056}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'layer_size':hp.quniform('layer_size', 25, 100, 1),\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         'learning_rate':hp.loguniform('learning_rate', low=np.log(1e-4), high=np.log(1.)),\n",
    "        }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='cancer_specific_mortality',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='NN',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 9/80 [02:41<21:43, 18.36s/trial, best loss: -0.8183074098461336] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 10/80 [02:56<19:57, 17.11s/trial, best loss: -0.8183074098461336]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 35/80 [20:48<1:08:38, 91.51s/trial, best loss: -0.8718663275528582]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 47/80 [32:47<20:57, 38.11s/trial, best loss: -0.8808997977206688]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 48/80 [33:02<16:29, 30.94s/trial, best loss: -0.8808997977206688]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 59/80 [42:56<27:01, 77.20s/trial, best loss: -0.8929078928729268]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 60/80 [43:12<19:38, 58.91s/trial, best loss: -0.8929078928729268]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 64/80 [47:41<20:16, 76.03s/trial, best loss: -0.8929078928729268]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 71/80 [1:06:30<25:18, 168.73s/trial, best loss: -0.8929078928729268]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 78/80 [1:15:15<01:35, 47.99s/trial, best loss: -0.8929078928729268] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [1:19:31<00:00, 59.65s/trial, best loss: -0.8929078928729268]\n",
      "{'activation': 2, 'alpha': 3.670399366093249e-06, 'hidden_layer_sizes': 4, 'learning_rate': 1, 'max_iter': 0, 'solver': 2}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'hidden_layer_sizes':hp.choice('hidden_layer_sizes', [8, 16, 32, (8,8), (16,16)]), # 8, 16, 32, (8,8), (16,16)\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         'learning_rate':hp.choice('learning_rate', ['constant','adaptive']),\n",
    "         'max_iter': hp.choice('max_iter', [500]),\n",
    "         'solver': hp.choice('solver', ['sgd', 'adam', 'lbfgs']),\n",
    "        }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='cancer_specific_mortality',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='NN',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 14/80 [21:59<1:22:30, 75.01s/trial, best loss: -0.8755981332541104] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 15/80 [22:15<1:01:49, 57.06s/trial, best loss: -0.8755981332541104]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 54/80 [47:40<21:54, 50.56s/trial, best loss: -0.8755981332541104]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 55/80 [47:55<16:42, 40.10s/trial, best loss: -0.8755981332541104]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [58:04<00:00, 43.56s/trial, best loss: -0.8755981332541104]\n",
      "{'activation': 1, 'alpha': 0.0002340750505402387, 'hidden_layer_sizes': 2, 'learning_rate': 0, 'max_iter': 0, 'solver': 1}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'hidden_layer_sizes':hp.choice('hidden_layer_sizes', [8, 16, 32, (8,8), (16,16), (32, 16), (32, 32)]), # 8, 16, 32, (8,8), (16,16)\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         'learning_rate':hp.choice('learning_rate', ['constant','adaptive']),\n",
    "         'max_iter': hp.choice('max_iter', [2000]),\n",
    "         'solver': hp.choice('solver', ['sgd', 'adam', 'lbfgs']),\n",
    "        }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='cancer_specific_mortality',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='NN',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 7/80 [14:01<1:36:48, 79.56s/trial, best loss: -0.8440756313487654] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n",
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█         | 8/80 [14:34<1:17:58, 64.97s/trial, best loss: -0.8440756313487654]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 77/80 [1:17:21<02:25, 48.42s/trial, best loss: -0.8678197664611105] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vytautas\\Studijos\\prostate-cancer-mortality\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [1:29:17<00:00, 66.96s/trial, best loss: -0.8678197664611105] \n",
      "{'activation': 1, 'alpha': 6.835502772513517e-05, 'hidden_layer_sizes': 11, 'learning_rate': 0, 'max_iter': 0, 'solver': 1}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'hidden_layer_sizes':hp.choice('hidden_layer_sizes', [8, 16, 32, 64, 128, \n",
    "                                                                     (8,8), (16,16), (32, 16), (32, 32),\n",
    "                                                                     (16, 8), (128, 64, 32, 16), (32, 16, 16)]), # 8, 16, 32, (8,8), (16,16)\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         'learning_rate':hp.choice('learning_rate', ['constant','adaptive']),\n",
    "         'max_iter': hp.choice('max_iter', [2500]),\n",
    "         'solver': hp.choice('solver', ['sgd', 'adam', 'lbfgs']),\n",
    "        }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='cancer_specific_mortality',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='NN',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### death from other causes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [47:16<00:00, 35.46s/trial, best loss: -0.5927730456162876]\n",
      "{'activation': 2, 'alpha': 0.0002616927777929669, 'layer_size': 70.0, 'learning_rate': 0.0008885558916449781}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'layer_size':hp.quniform('layer_size', 25, 100, 1),\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         'learning_rate':hp.loguniform('learning_rate', low=np.log(1e-4), high=np.log(1.)),\n",
    "         'max_iter': hp.choice('max_iter', [1000]),\n",
    "        }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='death_from_other_causes',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='NN',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [42:38<00:00, 31.98s/trial, best loss: -0.847685297272505] \n",
      "{'activation': 0, 'alpha': 0.005821356388227255, 'layer_size': 34.0, 'learning_rate': 0.0002702352915791481}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'layer_size':hp.quniform('layer_size', 25, 100, 1),\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         'learning_rate':hp.loguniform('learning_rate', low=np.log(1e-4), high=np.log(1.)),\n",
    "        }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='bcr',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='NN',\n",
    "                                              max_evals=80)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [32:44<00:00, 24.56s/trial, best loss: -0.8832526100502556] \n",
      "{'activation': 2, 'alpha': 1.0637827193154231e-05, 'layer_size': 45.0, 'learning_rate': 0.025594709065934664}\n"
     ]
    }
   ],
   "source": [
    "space_model = {'layer_size':hp.quniform('layer_size', 25, 100, 1),\n",
    "         'alpha':hp.lognormal('alpha', mu=np.log(1e-4), sigma=1),\n",
    "         'activation':hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "         'learning_rate':hp.loguniform('learning_rate', low=np.log(1e-4), high=np.log(1.)),\n",
    "        }\n",
    "\n",
    "trials, best = optimize_model_stratifiedkfold(target_column='mts',\n",
    "                                              max_time=216,\n",
    "                                              space=space_model,\n",
    "                                              df = data_train.copy(),\n",
    "                                              model_type='NN',\n",
    "                                              max_evals=80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
