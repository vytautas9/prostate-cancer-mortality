{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook to test and find best hyperparameters for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 25168\n",
      "Number of CPUs in the system: 16\n",
      "executing command 'taskset -cp 0,1,2,3,4,5,6,7,8,9,10,11,12,13 25168' ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pid = os.getpid()\n",
    "print(\"PID: %i\" % pid)\n",
    "\n",
    "n_cpu = os.cpu_count()   # Number of CPUs assigned to this process\n",
    "print(\"Number of CPUs in the system:\", n_cpu)\n",
    "\n",
    "# we won't use all the available cpu's for this script \n",
    "n_jobs = n_cpu - 2 # The number of tasks to run in parallel\n",
    "\n",
    "# Control which CPUs are made available for this script\n",
    "cpu_arg = ''.join([str(ci) + ',' for ci in list(range(n_jobs))])[:-1]\n",
    "cmd = 'taskset -cp %s %i' % (cpu_arg, pid)\n",
    "print(\"executing command '%s' ...\" % cmd)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to cleaned data file\n",
    "trainFilePath = 'data/data_train.pkl'\n",
    "\n",
    "# Read already prepared and saved train datasets\n",
    "with open(trainFilePath, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "# Create dummy variables for categorical data\n",
    "data_train = pd.get_dummies(data_train, columns=['clinical_stage', 'biopsy_gleason_gg', 'pathological_gleason_gg',\n",
    "                                'pathologic_stage', 'lni', 'surgical_margin_status', 'persistent_psa',\n",
    "                                'TRYSgrupes', 'PLNDO1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explodes the provided \"df\" dataset based on provided survival column \"time\" and\n",
    "clips the data to be in a range [min_time; max_time] (). A new discrete survival column\n",
    "will be created with name set as variable \"time_discrete\". \"cum_event\" boolean determines\n",
    "if cumulative event column will be created or no.\n",
    "\n",
    "clip(lower, upper) function will help us create a new discrete survival time column. \n",
    "If we specify lower=1 and upper=200, patients who experienced event earlier than 200th \n",
    "month will only have records till their event, on other side, if a patient survived past \n",
    "200th month, we will clip this information and will only keep information about him til 200th month.\n",
    "Another example, if we specify lower=140 and upper=200, and if the person experienced event \n",
    "at 100th month, we will create records for him till 140th (lower boundary) month.\n",
    "\"\"\"\n",
    "def explode_data(df,max_time,time,target_column,min_time=1,\n",
    "                 time_discrete='survival_time_discrete',cum_event=False):\n",
    "\n",
    "    target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "    # We create a new time column and clip the data by provided min and max survival times\n",
    "    df[time_discrete] = df[time].clip(min_time,max_time).apply(range)\n",
    "\n",
    "    # Exploding the dataset with the created range value in new time column\n",
    "    data_exploded = df.explode(time_discrete)\n",
    "    data_exploded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # New column starts at 0, we'll increase each value by 1\n",
    "    data_exploded[time_discrete] = pd.to_numeric(data_exploded[time_discrete]) + 1\n",
    "\n",
    "    # New event column, which will indicate the last event date\n",
    "    data_exploded[target_column_discrete] = (data_exploded[time_discrete] >= data_exploded[time]) * pd.to_numeric(data_exploded[target_column])\n",
    "    \n",
    "    if cum_event == True:\n",
    "        target_column_cumulative = target_column + '_cumulative'\n",
    "\n",
    "        # Create new event column with duplicated event values from discrete column\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_discrete]\n",
    "        \n",
    "        # For cumulative events, after end_time we will have NA values, we'll replace those with event indicator\n",
    "        after_survival_time = data_exploded[time_discrete] > data_exploded[time]\n",
    "        data_exploded.loc[after_survival_time, target_column_discrete] = -1\n",
    "        data_exploded[target_column_discrete] = data_exploded[target_column_discrete].replace(-1,np.NaN)\n",
    "        data_exploded.loc[(after_survival_time & (data_exploded[target_column]==0)), target_column_cumulative] = -1\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_cumulative].replace(-1,np.NaN)\n",
    "\n",
    "    return data_exploded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 200\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'psa', 'biopsy_gleason', 'pathologic_gleason',\n",
       "       'clinical_stage_1', 'clinical_stage_2', 'clinical_stage_3',\n",
       "       'biopsy_gleason_gg_1', 'biopsy_gleason_gg_2', 'biopsy_gleason_gg_3',\n",
       "       'biopsy_gleason_gg_4', 'biopsy_gleason_gg_5',\n",
       "       'pathological_gleason_gg_1', 'pathological_gleason_gg_2',\n",
       "       'pathological_gleason_gg_3', 'pathological_gleason_gg_4',\n",
       "       'pathological_gleason_gg_5', 'pathologic_stage_0', 'pathologic_stage_1',\n",
       "       'pathologic_stage_2', 'lni_0.0', 'lni_1.0', 'lni_unknown',\n",
       "       'surgical_margin_status_0', 'surgical_margin_status_1',\n",
       "       'persistent_psa_0', 'persistent_psa_1', 'TRYSgrupes_0', 'TRYSgrupes_1',\n",
       "       'TRYSgrupes_2', 'PLNDO1_0', 'PLNDO1_1', 'survival_time_discrete'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators':np.arange(150,700,50),\n",
    "              'max_features':np.arange(0.1, 1, 0.1),\n",
    "              'max_depth': [3, 5, 7, 9],\n",
    "              'max_samples': [0.3, 0.5, 0.8]}\n",
    "\n",
    "rf_random = RSCV(RandomForestClassifier(), param_grid, n_iter=15, \n",
    "             random_state=0, verbose=2, scoring='roc_auc', n_jobs=n_jobs)\n",
    "model_rf = rf_random.fit(X_train, y_train)\n",
    "model_rf_best = model_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 350,\n",
       " 'max_samples': 0.8,\n",
       " 'max_features': 0.7000000000000001,\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "model_rf.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
