{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook to test and find best hyperparameters for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV as RSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID: 11708\n",
      "Number of CPUs in the system: 16\n",
      "executing command 'taskset -cp 0,1,2,3,4,5,6,7,8,9,10,11,12,13 11708' ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pid = os.getpid()\n",
    "print(\"PID: %i\" % pid)\n",
    "\n",
    "n_cpu = os.cpu_count()   # Number of CPUs assigned to this process\n",
    "print(\"Number of CPUs in the system:\", n_cpu)\n",
    "\n",
    "# we won't use all the available cpu's for this script \n",
    "n_jobs = n_cpu - 2 # The number of tasks to run in parallel\n",
    "\n",
    "# Control which CPUs are made available for this script\n",
    "cpu_arg = ''.join([str(ci) + ',' for ci in list(range(n_jobs))])[:-1]\n",
    "cmd = 'taskset -cp %s %i' % (cpu_arg, pid)\n",
    "print(\"executing command '%s' ...\" % cmd)\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to cleaned data file\n",
    "trainFilePath = 'data/data_train.pkl'\n",
    "\n",
    "# Read already prepared and saved train datasets\n",
    "with open(trainFilePath, 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "\n",
    "# Create dummy variables for categorical data\n",
    "data_train = pd.get_dummies(data_train, columns=['clinical_stage', 'biopsy_gleason_gg', 'pathological_gleason_gg',\n",
    "                                'pathologic_stage', 'lni', 'surgical_margin_status', 'persistent_psa',\n",
    "                                'TRYSgrupes', 'PLNDO1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Explodes the provided \"df\" dataset based on provided survival column \"time\" and\n",
    "clips the data to be in a range [min_time; max_time] (). A new discrete survival column\n",
    "will be created with name set as variable \"time_discrete\". \"cum_event\" boolean determines\n",
    "if cumulative event column will be created or no.\n",
    "\n",
    "clip(lower, upper) function will help us create a new discrete survival time column. \n",
    "If we specify lower=1 and upper=200, patients who experienced event earlier than 200th \n",
    "month will only have records till their event, on other side, if a patient survived past \n",
    "200th month, we will clip this information and will only keep information about him til 200th month.\n",
    "Another example, if we specify lower=140 and upper=200, and if the person experienced event \n",
    "at 100th month, we will create records for him till 140th (lower boundary) month.\n",
    "\"\"\"\n",
    "def explode_data(df,max_time,time,target_column,min_time=1,\n",
    "                 time_discrete='survival_time_discrete',cum_event=False):\n",
    "\n",
    "    target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "    # We create a new time column and clip the data by provided min and max survival times\n",
    "    df[time_discrete] = df[time].clip(min_time,max_time).apply(range)\n",
    "\n",
    "    # Exploding the dataset with the created range value in new time column\n",
    "    data_exploded = df.explode(time_discrete)\n",
    "    data_exploded.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # New column starts at 0, we'll increase each value by 1\n",
    "    data_exploded[time_discrete] = pd.to_numeric(data_exploded[time_discrete]) + 1\n",
    "\n",
    "    # New event column, which will indicate the last event date\n",
    "    data_exploded[target_column_discrete] = (data_exploded[time_discrete] >= data_exploded[time]) * pd.to_numeric(data_exploded[target_column])\n",
    "    \n",
    "    if cum_event == True:\n",
    "        target_column_cumulative = target_column + '_cumulative'\n",
    "\n",
    "        # Create new event column with duplicated event values from discrete column\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_discrete]\n",
    "        \n",
    "        # For cumulative events, after end_time we will have NA values, we'll replace those with event indicator\n",
    "        after_survival_time = data_exploded[time_discrete] > data_exploded[time]\n",
    "        data_exploded.loc[after_survival_time, target_column_discrete] = -1\n",
    "        data_exploded[target_column_discrete] = data_exploded[target_column_discrete].replace(-1,np.NaN)\n",
    "        data_exploded.loc[(after_survival_time & (data_exploded[target_column]==0)), target_column_cumulative] = -1\n",
    "        data_exploded[target_column_cumulative] = data_exploded[target_column_cumulative].replace(-1,np.NaN)\n",
    "\n",
    "    return data_exploded\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given an exploded dataset with instant mortality probabilities \"event_probability_column\"\n",
    "and \"id_column\" for grouping (optional), cumulative hazard column will be calculated\n",
    "\"\"\"\n",
    "def cumulative_hazard(df, event_probability_column, id_column):\n",
    "    data_copy = df.copy()\n",
    "    if id_column is not None:\n",
    "        data_copy = data_copy[ [id_column, event_probability_column] ]\n",
    "    else:\n",
    "        data_copy = data_copy[ [event_probability_column] ]\n",
    "    data_copy['negative_log_prob'] = np.log( 1 - data_copy[event_probability_column] )\n",
    "    if id_column is not None:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy.groupby(id_column)['negative_log_prob'].transform(pd.Series.cumsum))\n",
    "    else:\n",
    "        data_copy['cumulative_hazard'] = 1 - np.exp(data_copy['negative_log_prob'].cumsum())\n",
    "    return data_copy['cumulative_hazard']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given non exploded dataset, explodes the dataset based on \"max_time\", lower boundary for exploding\n",
    "a dataset will be the same as upper, so for each patient we will create \"max_time\" records.\n",
    "Adds predictent instant mortality probabilities as well as cumulative ones.\n",
    "\"\"\"\n",
    "def add_predict_probabilities(df, model, max_time, target_column, time, is_homogenous_dataset=False):\n",
    "\n",
    "    df_exploded = explode_data(df, max_time=max_time, min_time=max_time, cum_event=True, time=time, target_column=target_column)\n",
    "\n",
    "    x_columns_to_drop = [target_column, target_column+'_discrete', target_column+'_cumulative', 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "    X_df = df_exploded.drop(x_columns_to_drop, axis=1)\n",
    "\n",
    "    # probabilities\n",
    "    y_pred = model.predict_proba(X_df)[:,1]\n",
    "    df_exploded['mortality_instant_prob'] = y_pred\n",
    "\n",
    "    # Cumulative hazard for each patient\n",
    "    df_exploded['cumulative_hazard'] = cumulative_hazard(df_exploded,'mortality_instant_prob','patient_id')\n",
    "    \n",
    "    # TODO - homogenous dataset\n",
    "    # if is_homogenous_dataset:\n",
    "        \n",
    "    #     # get adjustment parameters\n",
    "    #     match target_column:\n",
    "    #         case 'cancer_specific_mortality':\n",
    "    #             pi_0 = pi_0_csm\n",
    "    #             pi_1 = pi_1_csm\n",
    "    #             rho_0 = rho_0_csm\n",
    "    #             rho_1 = rho_1_csm\n",
    "    #         case 'death_from_other_causes':\n",
    "    #             pi_0 = pi_0_doc\n",
    "    #             pi_1 = pi_1_doc\n",
    "    #             rho_0 = rho_0_doc\n",
    "    #             rho_1 = rho_1_doc\n",
    "    #         case 'mts':\n",
    "    #             pi_0 = pi_0_mts\n",
    "    #             pi_1 = pi_1_mts\n",
    "    #             rho_0 = rho_0_mts\n",
    "    #             rho_1 = rho_1_mts\n",
    "    #         case 'bcr':\n",
    "    #             pi_0 = pi_0_bcr\n",
    "    #             pi_1 = pi_1_bcr\n",
    "    #             rho_0 = rho_0_bcr\n",
    "    #             rho_1 = rho_1_bcr\n",
    "    #         case _:\n",
    "    #             pi_0 = None\n",
    "    #             pi_1 = None\n",
    "    #             rho_0 = None\n",
    "    #             rho_1 = None\n",
    "        \n",
    "        \n",
    "    #     # adjusted probabilities\n",
    "    #     df_exploded['mortality_instant_prob_adjusted'] = \\\n",
    "    #         (df_exploded.mortality_instant_prob*(pi_1/rho_1)) / \\\n",
    "    #         ((1-df_exploded.mortality_instant_prob)*(pi_0/rho_0) + \\\n",
    "    #          df_exploded.mortality_instant_prob*(pi_1/rho_1))\n",
    "        \n",
    "    #     # Cumulative hazard for each patient (adjusted)\n",
    "    #     df_exploded['cumulative_hazard_adjusted'] = cumulative_hazard(df_exploded,'mortality_instant_prob_adjusted','patient_id')\n",
    "    \n",
    "    return df_exploded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 200\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'psa', 'biopsy_gleason', 'pathologic_gleason',\n",
       "       'clinical_stage_1', 'clinical_stage_2', 'clinical_stage_3',\n",
       "       'biopsy_gleason_gg_1', 'biopsy_gleason_gg_2', 'biopsy_gleason_gg_3',\n",
       "       'biopsy_gleason_gg_4', 'biopsy_gleason_gg_5',\n",
       "       'pathological_gleason_gg_1', 'pathological_gleason_gg_2',\n",
       "       'pathological_gleason_gg_3', 'pathological_gleason_gg_4',\n",
       "       'pathological_gleason_gg_5', 'pathologic_stage_0', 'pathologic_stage_1',\n",
       "       'pathologic_stage_2', 'lni_0.0', 'lni_1.0', 'lni_unknown',\n",
       "       'surgical_margin_status_0', 'surgical_margin_status_1',\n",
       "       'persistent_psa_0', 'persistent_psa_1', 'TRYSgrupes_0', 'TRYSgrupes_1',\n",
       "       'TRYSgrupes_2', 'PLNDO1_0', 'PLNDO1_1', 'survival_time_discrete'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators':np.arange(150,700,50),\n",
    "              'max_features':np.arange(0.1, 1, 0.1),\n",
    "              'max_depth': [3, 5, 7, 9],\n",
    "              'max_samples': [0.3, 0.5, 0.8],\n",
    "              'min_samples_leaf': [1, 2, 3, 4, 5]}\n",
    "\n",
    "rf_random = RSCV(RandomForestClassifier(), param_grid, n_iter=15, \n",
    "             random_state=0, verbose=2, scoring='roc_auc', n_jobs=n_jobs)\n",
    "model_rf = rf_random.fit(X_train, y_train)\n",
    "model_rf_best = model_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_leaf': 5,\n",
       " 'max_samples': 0.3,\n",
       " 'max_features': 0.5,\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "model_rf.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cancer specific mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'cancer_specific_mortality'\n",
    "max_time = 200\n",
    "\n",
    "# mts and bcr have different survival months columns\n",
    "match target_column:\n",
    "    case 'mts':\n",
    "        time = 'survival_months_mts'\n",
    "    case 'bcr':\n",
    "        time = 'survival_months_bcr'\n",
    "    case _:\n",
    "        time = 'survival_months'\n",
    "\n",
    "target_column_discrete = target_column + '_discrete'\n",
    "\n",
    "# List of columns names which will be dropped from feature set before fitting the model\n",
    "target_columns = ['cancer_specific_mortality', 'death_from_other_causes', 'bcr', 'mts']\n",
    "\n",
    "# Explode the dataset\n",
    "df_train_copy_exploded = explode_data(data_train.copy(), min_time=1, max_time=max_time, time=time, target_column=target_column)\n",
    "\n",
    "# Drop targets/features from feature set\n",
    "x_columns_to_drop = [target_column, target_column_discrete, 'survival_months', 'survival_months_bcr', 'survival_months_mts', 'patient_id']\n",
    "x_columns_to_drop.extend(target_columns)\n",
    "X_train = df_train_copy_exploded.drop(x_columns_to_drop, axis=1)    \n",
    "y_train = df_train_copy_exploded[target_column_discrete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.uniform('max_depth',5,20),\n",
    "        'max_features': hp.choice('max_features', ['sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.uniform('n_estimators', 100, 500)\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = int(space['max_depth']),\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = int(space['n_estimators']), \n",
    "                                 )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #auc = cross_val_score(model, X_train, y_train, cv = 5, scoring='roc_auc').mean()\n",
    "    # Test on training data\n",
    "    data_train_copy = data_train.copy()\n",
    "    target_columns_to_drop = target_columns.copy()\n",
    "    target_columns_to_drop.remove(target_column)\n",
    "    data_train_copy = data_train_copy.drop(target_columns_to_drop, axis=1)\n",
    "    df_train_predicted = add_predict_probabilities(data_train_copy, model=model, max_time=max_time, \n",
    "                                                     target_column=target_column, time=time)\n",
    "\n",
    "    # AUC for each cumulative slice\n",
    "    # Months at which we'll check the AUC's\n",
    "    months = list(range(6, max_time, 6))\n",
    "\n",
    "    train_auc_stats = []\n",
    "    for month in months:\n",
    "        # --- Training data ---\n",
    "        # Selecting a subset of data based on the months\n",
    "        select = (df_train_predicted['survival_time_discrete'] == month) & pd.notna(df_train_predicted[target_column+'_cumulative'])\n",
    "        sub_dat = df_train_predicted[select]\n",
    "\n",
    "        # If in the sliced data there's a event, calculate AUC metric,\n",
    "        # otherwise assign NaN value\n",
    "        if sub_dat[target_column+'_cumulative'].max() == 1:\n",
    "            fpr, tpr, thresholds = roc_curve(sub_dat[target_column+'_cumulative'], sub_dat['cumulative_hazard'])\n",
    "            auc_stat = auc(fpr, tpr)\n",
    "        else:\n",
    "            auc_stat = float('NaN')\n",
    "        train_auc_stats.append(auc_stat)\n",
    "\n",
    "    auc_mean = np.nanmean(train_auc_stats)\n",
    "\n",
    "    # We aim to maximize auc, therefore we return it as a negative value\n",
    "    return {'loss': -auc_mean, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [11:22<00:00, 17.07s/trial, best loss: -0.9258064473457128]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 18.61491475813544,\n",
       " 'max_features': 2,\n",
       " 'min_samples_leaf': 5.4715419273109606e-05,\n",
       " 'min_samples_split': 0.3178996881755699,\n",
       " 'n_estimators': 239.84475133029923}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 40, #80\n",
    "            trials = trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 1,\n",
       " 'max_depth': 200.0,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 0.043813847274908446,\n",
       " 'min_samples_split': 0.551636834827235,\n",
       " 'n_estimators': 5}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
